# LLM æä¾›è€… API æ–‡æª” - LegalStatuteAnalysis_V1

> **æ–‡ä»¶ç‰ˆæœ¬**ï¼š1.0
> **æœ€å¾Œæ›´æ–°**ï¼š2025-09-23
> **æ¨¡çµ„è·¯å¾‘**ï¼š`src.main.python.services.llm`
> **ç‹€æ…‹**ï¼šæ´»èº

---

## ğŸ“‹ æ¦‚è¦½

LLM æä¾›è€…æ¨¡çµ„å¯¦ç¾çµ±ä¸€çš„å¤š LLM æ”¯æ´ï¼Œéµå¾ª Linus "æ¶ˆé™¤ç‰¹æ®Šæƒ…æ³" åŸå‰‡ï¼Œæ‰€æœ‰æä¾›è€…éƒ½å¯¦ç¾ç›¸åŒä»‹é¢ï¼Œç¢ºä¿åˆ‡æ›ç„¡ç—›ä¸”æ¥­å‹™é‚è¼¯ä¸è®Šã€‚

---

## ğŸ¯ çµ±ä¸€ä»‹é¢è¨­è¨ˆ

### LLMProvider æŠ½è±¡åŸºé¡

**è¨­è¨ˆç†å¿µï¼š** ä¸€å€‹ä»‹é¢çµ±æ²»æ‰€æœ‰ LLMï¼Œç„¡ç‰¹æ®Šæƒ…æ³

```python
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional

class LLMProvider(ABC):
    """
    LLM æä¾›è€…çµ±ä¸€ä»‹é¢ - Linus å¼è¨­è¨ˆ

    åŸå‰‡ï¼š
    - å–®ä¸€ä»‹é¢ï¼šæ‰€æœ‰ LLM éƒ½å¯¦ç¾ç›¸åŒæ–¹æ³•
    - ç„¡ç‰¹æ®Šæƒ…æ³ï¼šä¸åŒæä¾›è€…çš„å·®ç•°åœ¨å¯¦ç¾å…§éƒ¨è™•ç†
    - ç°¡æ½”æ˜ç­ï¼šåªæä¾›å¿…è¦çš„æ–¹æ³•ï¼Œä¸éåº¦è¨­è¨ˆ
    """

    @abstractmethod
    async def analyze(self, prompt: str) -> str:
        """
        æ ¸å¿ƒåˆ†ææ–¹æ³• - å”¯ä¸€éœ€è¦å¯¦ç¾çš„æ¥­å‹™é‚è¼¯

        Args:
            prompt: åˆ†ææç¤ºè©

        Returns:
            str: LLM å›æ‡‰çš„ JSON å­—ä¸²

        Raises:
            LLMProviderError: LLM èª¿ç”¨å¤±æ•—æ™‚æ‹‹å‡º
        """
        pass

    @abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """
        å–å¾—æ¨¡å‹è³‡è¨Š

        Returns:
            Dict: åŒ…å« provider, model, version ç­‰è³‡è¨Š
        """
        pass

    @property
    @abstractmethod
    def is_available(self) -> bool:
        """æª¢æŸ¥æä¾›è€…æ˜¯å¦å¯ç”¨ï¼ˆAPI é‡‘é‘°ã€ç¶²è·¯ç­‰ï¼‰"""
        pass
```

---

## ğŸ¤– å…·é«”å¯¦ç¾

### OpenAIProvider - GPT ç³»åˆ—

**ç‰¹è‰²ï¼š** æ”¯æ´ GPT-3.5/GPT-4 ç³»åˆ—æ¨¡å‹

```python
from openai import AsyncOpenAI
from src.main.python.models import LLMConfig

class OpenAIProvider(LLMProvider):
    """OpenAI GPT ç³»åˆ—æä¾›è€… - å¯¦ç”¨ä¸»ç¾©å¯¦ç¾"""

    def __init__(self, config: LLMConfig):
        """
        åˆå§‹åŒ– OpenAI æä¾›è€…

        Args:
            config: LLM é…ç½®ï¼ŒåŒ…å«æ¨¡å‹ã€API é‡‘é‘°ç­‰è³‡è¨Š
        """
        self.config = config
        self.client = AsyncOpenAI(
            api_key=os.getenv(config.api_key_env),
            base_url=config.base_url
        )

    async def analyze(self, prompt: str) -> str:
        """ä½¿ç”¨ GPT æ¨¡å‹é€²è¡Œåˆ†æ"""
        try:
            response = await self.client.chat.completions.create(
                model=self.config.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ³•è¦åˆ†æå°ˆå®¶ï¼Œè«‹æ ¹æ“šå•é¡Œå…§å®¹åˆ†æç›¸é—œæ³•æ¢ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )

            return response.choices[0].message.content

        except Exception as e:
            raise LLMProviderError(f"OpenAI API èª¿ç”¨å¤±æ•—: {e}")

    def get_model_info(self) -> Dict[str, Any]:
        """å–å¾— OpenAI æ¨¡å‹è³‡è¨Š"""
        return {
            "provider": "openai",
            "model": self.config.model,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "version": "1.0"
        }

    @property
    def is_available(self) -> bool:
        """æª¢æŸ¥ OpenAI æ˜¯å¦å¯ç”¨"""
        return os.getenv(self.config.api_key_env) is not None
```

**æ”¯æ´çš„æ¨¡å‹ï¼š**
- `gpt-4o-mini` - æˆæœ¬æ•ˆç›Šæœ€ä½³ï¼Œæ¨è–¦ç”¨æ–¼å¤§é‡åˆ†æ
- `gpt-4o` - æœ€é«˜å“è³ªï¼Œé©åˆè¤‡é›œæ³•æ¢è§£æ
- `gpt-3.5-turbo` - å¿«é€ŸéŸ¿æ‡‰ï¼Œé©åˆç°¡å–®åŒ¹é…

**ä½¿ç”¨ç¯„ä¾‹ï¼š**

```python
from src.main.python.models import LLMConfig, LLMProvider as LLMProviderEnum

# é…ç½® OpenAI
openai_config = LLMConfig(
    provider=LLMProviderEnum.OPENAI,
    model="gpt-4o-mini",
    temperature=0.0,
    max_tokens=4000,
    api_key_env="OPENAI_API_KEY"
)

# å»ºç«‹æä¾›è€…
provider = OpenAIProvider(openai_config)

# æª¢æŸ¥å¯ç”¨æ€§
if provider.is_available:
    response = await provider.analyze("åˆ†æé€™å€‹æ³•æ¢å•é¡Œ...")
    print(response)
```

---

### ClaudeProvider - Anthropic Claude

**ç‰¹è‰²ï¼š** æ”¯æ´ Claude 3 ç³»åˆ—æ¨¡å‹

```python
import anthropic

class ClaudeProvider(LLMProvider):
    """Anthropic Claude æä¾›è€…"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self.client = anthropic.AsyncAnthropic(
            api_key=os.getenv(config.api_key_env)
        )

    async def analyze(self, prompt: str) -> str:
        """ä½¿ç”¨ Claude æ¨¡å‹é€²è¡Œåˆ†æ"""
        try:
            response = await self.client.messages.create(
                model=self.config.model,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                system="ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ³•è¦åˆ†æå°ˆå®¶ï¼Œè«‹æ ¹æ“šå•é¡Œå…§å®¹åˆ†æç›¸é—œæ³•æ¢ã€‚",
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )

            return response.content[0].text

        except Exception as e:
            raise LLMProviderError(f"Claude API èª¿ç”¨å¤±æ•—: {e}")

    def get_model_info(self) -> Dict[str, Any]:
        """å–å¾— Claude æ¨¡å‹è³‡è¨Š"""
        return {
            "provider": "claude",
            "model": self.config.model,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "version": "1.0"
        }

    @property
    def is_available(self) -> bool:
        """æª¢æŸ¥ Claude æ˜¯å¦å¯ç”¨"""
        return os.getenv(self.config.api_key_env) is not None
```

**æ”¯æ´çš„æ¨¡å‹ï¼š**
- `claude-3-haiku-20240307` - æœ€å¿«éŸ¿æ‡‰ï¼Œé©åˆæ‰¹é‡è™•ç†
- `claude-3-sonnet-20240229` - å¹³è¡¡æ€§èƒ½å’Œå“è³ª
- `claude-3-opus-20240229` - æœ€é«˜å“è³ªï¼Œé©åˆè¤‡é›œåˆ†æ

---

### GeminiProvider - Google Gemini

**ç‰¹è‰²ï¼š** æ”¯æ´ Google Gemini Pro å’Œ embedding æ¨¡å‹

```python
import google.generativeai as genai

class GeminiProvider(LLMProvider):
    """Google Gemini æä¾›è€…"""

    def __init__(self, config: LLMConfig):
        self.config = config
        genai.configure(api_key=os.getenv(config.api_key_env))
        self.model = genai.GenerativeModel(config.model)

    async def analyze(self, prompt: str) -> str:
        """ä½¿ç”¨ Gemini æ¨¡å‹é€²è¡Œåˆ†æ"""
        try:
            response = await self.model.generate_content_async(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=self.config.temperature,
                    max_output_tokens=self.config.max_tokens
                )
            )

            return response.text

        except Exception as e:
            raise LLMProviderError(f"Gemini API èª¿ç”¨å¤±æ•—: {e}")

    def get_model_info(self) -> Dict[str, Any]:
        """å–å¾— Gemini æ¨¡å‹è³‡è¨Š"""
        return {
            "provider": "gemini",
            "model": self.config.model,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "version": "1.0"
        }

    @property
    def is_available(self) -> bool:
        """æª¢æŸ¥ Gemini æ˜¯å¦å¯ç”¨"""
        return os.getenv(self.config.api_key_env) is not None
```

**æ”¯æ´çš„æ¨¡å‹ï¼š**
- `gemini-1.5-flash` - å¿«é€ŸéŸ¿æ‡‰ï¼Œæˆæœ¬æ•ˆç›Šä½³
- `gemini-1.5-pro` - é«˜å“è³ªåˆ†æï¼Œæ”¯æ´é•·æ–‡æœ¬
- `models/embedding-001` - å°ˆç”¨æ–¼ embedding è¨ˆç®—

---

### SimulationProvider - æ¸¬è©¦æ¨¡æ“¬å™¨

**ç‰¹è‰²ï¼š** æœ¬åœ°æ¨¡æ“¬ï¼Œç„¡éœ€ API é‡‘é‘°ï¼Œç”¨æ–¼é–‹ç™¼æ¸¬è©¦

```python
import json
import random
import asyncio

class SimulationProvider(LLMProvider):
    """
    æ¨¡æ“¬æä¾›è€… - Linus å¼å¯¦ç”¨ä¸»ç¾©

    ç”¨é€”ï¼š
    - é–‹ç™¼éšæ®µæ¸¬è©¦
    - ç„¡ API é‡‘é‘°æ™‚çš„å¾Œå‚™é¸é …
    - æ€§èƒ½åŸºæº–æ¸¬è©¦
    - æ¼”ç¤ºå’Œæ•™å­¸
    """

    def __init__(self, config: LLMConfig):
        self.config = config
        self.mock_articles = [
            "REA-ACT-13", "REA-ACT-14", "REA-ACT-15",
            "REA-ACT-20", "REA-ACT-21", "REA-ACT-25"
        ]

    async def analyze(self, prompt: str) -> str:
        """ç”¢ç”Ÿæ¨¡æ“¬çš„åˆ†æçµæœ"""

        # æ¨¡æ“¬ API å»¶é²
        await asyncio.sleep(random.uniform(0.5, 2.0))

        # åˆ†ææç¤ºè©è¤‡é›œåº¦ï¼Œæ±ºå®šä¿¡å¿ƒåº¦
        complexity_score = self._analyze_prompt_complexity(prompt)

        # ç”¢ç”Ÿæ¨¡æ“¬çµæœ
        mock_result = {
            "confidence": min(0.95, max(0.3, complexity_score + random.uniform(-0.2, 0.2))),
            "matched_articles": random.sample(self.mock_articles, random.randint(1, 3)),
            "primary_article": random.choice(self.mock_articles),
            "reasoning": f"æ¨¡æ“¬åˆ†æçµæœï¼šæ ¹æ“šå•é¡Œé—œéµè©åˆ†æï¼Œè­˜åˆ¥å‡ºèˆ‡{random.choice(['ç‡Ÿæ¥­è¦ç¯„', 'ç®¡ç†æ¢ä¾‹', 'æ­ç¤ºç¾©å‹™', 'åŸ·æ¥­è¦å‰‡'])}ç›¸é—œçš„æ³•æ¢è¦å®šã€‚"
        }

        return json.dumps(mock_result, ensure_ascii=False)

    def _analyze_prompt_complexity(self, prompt: str) -> float:
        """åˆ†ææç¤ºè©è¤‡é›œåº¦ - ç°¡å–®çš„å•Ÿç™¼å¼æ–¹æ³•"""
        if len(prompt) < 50:
            return 0.4  # çŸ­å•é¡Œï¼Œä½ä¿¡å¿ƒåº¦
        elif len(prompt) > 200:
            return 0.8  # é•·å•é¡Œï¼Œé«˜ä¿¡å¿ƒåº¦
        else:
            return 0.6  # ä¸­ç­‰å•é¡Œï¼Œä¸­ç­‰ä¿¡å¿ƒåº¦

    def get_model_info(self) -> Dict[str, Any]:
        """å–å¾—æ¨¡æ“¬å™¨è³‡è¨Š"""
        return {
            "provider": "simulation",
            "model": "mock-v1.0",
            "temperature": 0.0,
            "max_tokens": 4000,
            "version": "1.0",
            "description": "æœ¬åœ°æ¨¡æ“¬å™¨ï¼Œç”¨æ–¼é–‹ç™¼æ¸¬è©¦"
        }

    @property
    def is_available(self) -> bool:
        """æ¨¡æ“¬å™¨æ°¸é å¯ç”¨"""
        return True
```

---

## ğŸ”§ æä¾›è€…å·¥å» 

### LLMProviderFactory - çµ±ä¸€å»ºç«‹ä»‹é¢

**ç”¨é€”ï¼š** æ ¹æ“šé…ç½®è‡ªå‹•å»ºç«‹å°æ‡‰çš„ LLM æä¾›è€…

```python
from src.main.python.models import LLMConfig, LLMProvider as LLMProviderEnum

class LLMProviderFactory:
    """
    LLM æä¾›è€…å·¥å»  - æ¶ˆé™¤å»ºç«‹æä¾›è€…çš„ç‰¹æ®Šæƒ…æ³

    Linus åŸå‰‡é«”ç¾ï¼š
    - çµ±ä¸€å…¥å£ï¼šæ‰€æœ‰æä¾›è€…é€šéåŒä¸€æ–¹æ³•å»ºç«‹
    - ç„¡ç‰¹æ®Šé‚è¼¯ï¼šæ·»åŠ æ–°æä¾›è€…åªéœ€è¨»å†Šä¸€è¡Œ
    - å¿«é€Ÿå¤±æ•—ï¼šä¸æ”¯æ´çš„æä¾›è€…ç«‹å³å ±éŒ¯
    """

    _PROVIDERS = {
        LLMProviderEnum.OPENAI: OpenAIProvider,
        LLMProviderEnum.CLAUDE: ClaudeProvider,
        LLMProviderEnum.GEMINI: GeminiProvider,
        LLMProviderEnum.SIMULATION: SimulationProvider,
    }

    @classmethod
    def create_provider(cls, config: LLMConfig) -> LLMProvider:
        """
        å»ºç«‹ LLM æä¾›è€…

        Args:
            config: LLM é…ç½®

        Returns:
            LLMProvider: å°æ‡‰çš„æä¾›è€…å¯¦ä¾‹

        Raises:
            ValueError: ä¸æ”¯æ´çš„æä¾›è€…é¡å‹
        """
        provider_class = cls._PROVIDERS.get(config.provider)
        if not provider_class:
            supported = list(cls._PROVIDERS.keys())
            raise ValueError(f"ä¸æ”¯æ´çš„ LLM æä¾›è€…: {config.provider}, æ”¯æ´çš„æä¾›è€…: {supported}")

        return provider_class(config)

    @classmethod
    def get_available_providers(cls) -> List[LLMProviderEnum]:
        """å–å¾—æ‰€æœ‰å¯ç”¨çš„æä¾›è€…åˆ—è¡¨"""
        return list(cls._PROVIDERS.keys())

    @classmethod
    def register_provider(cls, provider_type: LLMProviderEnum, provider_class: type):
        """
        è¨»å†Šæ–°çš„æä¾›è€… - æ“´å±•ç³»çµ±çš„å”¯ä¸€æ–¹æ³•

        Args:
            provider_type: æä¾›è€…é¡å‹æšèˆ‰
            provider_class: æä¾›è€…å¯¦ç¾é¡åˆ¥
        """
        cls._PROVIDERS[provider_type] = provider_class
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**

```python
from src.main.python.models import SystemConfig

# è¼‰å…¥é…ç½®
config = SystemConfig.load_from_file("config/law_config.json")
openai_config = config.get_llm_config("openai")

# å»ºç«‹æä¾›è€…
provider = LLMProviderFactory.create_provider(openai_config)

# æª¢æŸ¥å¯ç”¨æ€§ä¸¦ä½¿ç”¨
if provider.is_available:
    result = await provider.analyze("æ³•æ¢åˆ†æå•é¡Œ...")
    model_info = provider.get_model_info()
    print(f"ä½¿ç”¨æ¨¡å‹: {model_info['model']}")
```

---

## ğŸ›¡ï¸ éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶

### ç•°å¸¸é¡å‹

```python
class LLMProviderError(Exception):
    """LLM æä¾›è€…åŸºç¤ç•°å¸¸"""
    pass

class APIKeyError(LLMProviderError):
    """API é‡‘é‘°ç›¸é—œéŒ¯èª¤"""
    pass

class RateLimitError(LLMProviderError):
    """API é€Ÿç‡é™åˆ¶éŒ¯èª¤"""
    pass

class ModelNotAvailableError(LLMProviderError):
    """æ¨¡å‹ä¸å¯ç”¨éŒ¯èª¤"""
    pass
```

### é‡è©¦æ©Ÿåˆ¶

```python
import asyncio
from typing import Optional

class RetryableProvider:
    """
    å¸¶é‡è©¦æ©Ÿåˆ¶çš„æä¾›è€…åŒ…è£å™¨ - ç°¡å–®çš„æŒ‡æ•¸é€€é¿

    Linus åŸå‰‡ï¼š
    - ç°¡å–®é‡è©¦ï¼šä¸åšè¤‡é›œçš„é‡è©¦ç­–ç•¥
    - å¿«é€Ÿå¤±æ•—ï¼šè¶…éé‡è©¦æ¬¡æ•¸ç«‹å³æ‹‹å‡ºç•°å¸¸
    - å¯¦ç”¨ä¸»ç¾©ï¼šåªè™•ç†å¸¸è¦‹çš„æš«æ™‚æ€§éŒ¯èª¤
    """

    def __init__(self, provider: LLMProvider, max_retries: int = 3):
        self.provider = provider
        self.max_retries = max_retries

    async def analyze_with_retry(self, prompt: str) -> str:
        """å¸¶é‡è©¦æ©Ÿåˆ¶çš„åˆ†æ"""
        last_error = None

        for attempt in range(self.max_retries + 1):
            try:
                return await self.provider.analyze(prompt)

            except RateLimitError as e:
                last_error = e
                if attempt < self.max_retries:
                    wait_time = 2 ** attempt  # æŒ‡æ•¸é€€é¿ï¼š2, 4, 8 ç§’
                    logger.warning(f"é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    break

            except (APIKeyError, ModelNotAvailableError):
                # é€™äº›éŒ¯èª¤ä¸éœ€è¦é‡è©¦
                raise

            except LLMProviderError as e:
                last_error = e
                if attempt < self.max_retries:
                    wait_time = 1 + random.uniform(0, 1)  # éš¨æ©Ÿç­‰å¾… 1-2 ç§’
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    break

        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
        raise LLMProviderError(f"æ‰€æœ‰é‡è©¦å¤±æ•—ï¼Œæœ€å¾ŒéŒ¯èª¤: {last_error}")
```

---

## ğŸ“Š æ€§èƒ½ç›£æ§

### æä¾›è€…æ€§èƒ½è¿½è¹¤

```python
from dataclasses import dataclass
from datetime import datetime
from typing import List

@dataclass
class ProviderMetrics:
    """æä¾›è€…æ€§èƒ½æŒ‡æ¨™"""
    provider_name: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    average_response_time: float
    total_tokens_used: Optional[int] = None
    total_cost: Optional[float] = None

class ProviderMonitor:
    """LLM æä¾›è€…æ€§èƒ½ç›£æ§"""

    def __init__(self):
        self.metrics: Dict[str, ProviderMetrics] = {}

    async def monitored_analyze(self, provider: LLMProvider, prompt: str) -> str:
        """ç›£æ§åŒ…è£çš„åˆ†ææ–¹æ³•"""
        provider_name = provider.get_model_info()["provider"]
        start_time = time.time()

        try:
            result = await provider.analyze(prompt)
            response_time = time.time() - start_time

            # è¨˜éŒ„æˆåŠŸ
            self._record_success(provider_name, response_time)
            return result

        except Exception as e:
            response_time = time.time() - start_time

            # è¨˜éŒ„å¤±æ•—
            self._record_failure(provider_name, response_time)
            raise

    def get_provider_metrics(self, provider_name: str) -> Optional[ProviderMetrics]:
        """å–å¾—æŒ‡å®šæä¾›è€…çš„æ€§èƒ½æŒ‡æ¨™"""
        return self.metrics.get(provider_name)

    def get_all_metrics(self) -> Dict[str, ProviderMetrics]:
        """å–å¾—æ‰€æœ‰æä¾›è€…çš„æ€§èƒ½æŒ‡æ¨™"""
        return self.metrics.copy()
```

---

## ğŸ”„ æä¾›è€…åˆ‡æ›ç­–ç•¥

### è‡ªå‹•é™ç´šæ©Ÿåˆ¶

```python
class FallbackProvider:
    """
    è‡ªå‹•é™ç´šæä¾›è€… - Linus å¼å¯¦ç”¨ä¸»ç¾©

    ç­–ç•¥ï¼š
    1. å˜—è©¦ä¸»è¦æä¾›è€…
    2. å¤±æ•—æ™‚è‡ªå‹•åˆ‡æ›åˆ°å‚™ç”¨æä¾›è€…
    3. è¨˜éŒ„åˆ‡æ›åŸå› ä¾›å¾ŒçºŒåˆ†æ
    """

    def __init__(self, primary: LLMProvider, fallback: LLMProvider):
        self.primary = primary
        self.fallback = fallback
        self.current_provider = primary
        self.fallback_count = 0

    async def analyze(self, prompt: str) -> str:
        """å¸¶é™ç´šæ©Ÿåˆ¶çš„åˆ†æ"""

        # å˜—è©¦ä¸»è¦æä¾›è€…
        try:
            return await self.primary.analyze(prompt)
        except LLMProviderError as e:
            logger.warning(f"ä¸»è¦æä¾›è€…å¤±æ•—ï¼Œåˆ‡æ›åˆ°å‚™ç”¨æä¾›è€…: {e}")

            # åˆ‡æ›åˆ°å‚™ç”¨æä¾›è€…
            try:
                self.fallback_count += 1
                return await self.fallback.analyze(prompt)
            except LLMProviderError as fallback_error:
                raise LLMProviderError(f"ä¸»è¦å’Œå‚™ç”¨æä¾›è€…éƒ½å¤±æ•— - ä¸»è¦: {e}, å‚™ç”¨: {fallback_error}")

    def get_fallback_stats(self) -> Dict[str, Any]:
        """å–å¾—é™ç´šçµ±è¨ˆ"""
        return {
            "fallback_count": self.fallback_count,
            "primary_provider": self.primary.get_model_info()["provider"],
            "fallback_provider": self.fallback.get_model_info()["provider"]
        }
```

---

## ğŸ¯ æœ€ä½³å¯¦è¸ç¯„ä¾‹

### å®Œæ•´ä½¿ç”¨æµç¨‹

```python
async def complete_llm_workflow_example():
    """å®Œæ•´çš„ LLM ä½¿ç”¨æµç¨‹ç¯„ä¾‹"""

    # 1. è¼‰å…¥é…ç½®
    config = SystemConfig.load_from_file("config/law_config.json")

    # 2. å»ºç«‹ä¸»è¦å’Œå‚™ç”¨æä¾›è€…
    primary_config = config.get_llm_config("openai")
    fallback_config = config.get_llm_config("simulation")

    primary_provider = LLMProviderFactory.create_provider(primary_config)
    fallback_provider = LLMProviderFactory.create_provider(fallback_config)

    # 3. å»ºç«‹å¸¶é‡è©¦å’Œé™ç´šæ©Ÿåˆ¶çš„æä¾›è€…
    retryable_primary = RetryableProvider(primary_provider, max_retries=3)
    fallback_system = FallbackProvider(retryable_primary, fallback_provider)

    # 4. å»ºç«‹ç›£æ§
    monitor = ProviderMonitor()

    # 5. åŸ·è¡Œåˆ†æ
    prompt = "åˆ†æä»¥ä¸‹æ³•æ¢å•é¡Œ..."

    try:
        result = await monitor.monitored_analyze(fallback_system, prompt)
        print(f"åˆ†ææˆåŠŸ: {result}")

        # 6. æª¢æŸ¥æ€§èƒ½æŒ‡æ¨™
        metrics = monitor.get_all_metrics()
        for provider_name, provider_metrics in metrics.items():
            print(f"{provider_name}: æˆåŠŸç‡ {provider_metrics.successful_requests/provider_metrics.total_requests:.2%}")

    except LLMProviderError as e:
        print(f"æ‰€æœ‰æä¾›è€…éƒ½å¤±æ•—: {e}")
```

### æ–°æä¾›è€…é›†æˆç¯„ä¾‹

```python
# å‡è¨­è¦æ·»åŠ ä¸€å€‹æ–°çš„æœ¬åœ° LLM æä¾›è€…
class LocalLlamaProvider(LLMProvider):
    """æœ¬åœ° Llama æ¨¡å‹æä¾›è€…"""

    def __init__(self, config: LLMConfig):
        self.config = config
        # åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹...

    async def analyze(self, prompt: str) -> str:
        """ä½¿ç”¨æœ¬åœ° Llama æ¨¡å‹åˆ†æ"""
        # å¯¦ç¾æœ¬åœ°æ¨ç†é‚è¼¯...
        pass

    def get_model_info(self) -> Dict[str, Any]:
        return {
            "provider": "local_llama",
            "model": self.config.model,
            "version": "1.0"
        }

    @property
    def is_available(self) -> bool:
        return True  # æœ¬åœ°æ¨¡å‹ï¼Œç¸½æ˜¯å¯ç”¨

# è¨»å†Šæ–°æä¾›è€…
LLMProviderFactory.register_provider(
    LLMProviderEnum.LOCAL,  # ä½¿ç”¨ç¾æœ‰çš„ LOCAL æšèˆ‰
    LocalLlamaProvider
)
```

---

## ğŸ“ˆ æ€§èƒ½å°æ¯”åƒè€ƒ

### å„æä¾›è€…ç‰¹æ€§æ¯”è¼ƒ

| æä¾›è€… | éŸ¿æ‡‰é€Ÿåº¦ | åˆ†æå“è³ª | æˆæœ¬æ•ˆç›Š | æ¨è–¦å ´æ™¯ |
|-------|----------|----------|----------|-----------|
| GPT-4o-mini | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | å¤§é‡æ‰¹é‡åˆ†æ |
| GPT-4o | â­â­â­ | â­â­â­â­â­ | â­â­â­ | è¤‡é›œæ³•æ¢è§£æ |
| Claude Haiku | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | å¿«é€ŸéŸ¿æ‡‰éœ€æ±‚ |
| Claude Sonnet | â­â­â­â­ | â­â­â­â­ | â­â­â­ | å¹³è¡¡æ€§èƒ½å“è³ª |
| Gemini Flash | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | æˆæœ¬æ•æ„Ÿå ´æ™¯ |
| Simulation | â­â­â­â­â­ | â­ | â­â­â­â­â­ | é–‹ç™¼æ¸¬è©¦ |

---

## ğŸ¯ è¨­è¨ˆåŸå‰‡ç¸½çµ

**Linus å“²å­¸åœ¨ LLM æä¾›è€…ä¸­çš„é«”ç¾ï¼š**

**1. æ¶ˆé™¤ç‰¹æ®Šæƒ…æ³**
```python
# âœ… çµ±ä¸€ä»‹é¢ï¼Œç„¡ç‰¹æ®Šè™•ç†
async def analyze_with_any_llm(provider: LLMProvider, prompt: str) -> str:
    return await provider.analyze(prompt)  # æ‰€æœ‰æä¾›è€…éƒ½ä¸€æ¨£

# âŒ å……æ»¿ç‰¹æ®Šæƒ…æ³çš„èˆŠè¨­è¨ˆ
def analyze_with_specific_handling(provider_type, prompt):
    if provider_type == "openai":
        # OpenAI ç‰¹æ®Šè™•ç†...
    elif provider_type == "claude":
        # Claude ç‰¹æ®Šè™•ç†...
    # ... æ›´å¤šç‰¹æ®Šæƒ…æ³
```

**2. å¯¦ç”¨ä¸»ç¾©å„ªå…ˆ**
- å„ªå…ˆæ”¯æ´ä¸»æµã€ç©©å®šçš„ LLM API
- æä¾›æ¨¡æ“¬å™¨ç”¨æ–¼é–‹ç™¼æ¸¬è©¦
- ç°¡å–®çš„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶

**3. ç°¡æ½”ä»‹é¢**
- æ¯å€‹æä¾›è€…åªéœ€å¯¦ç¾ä¸‰å€‹æ–¹æ³•
- å·¥å» æ¨¡å¼çµ±ä¸€å»ºç«‹æµç¨‹
- æœ€å°‘ä½†è¶³å¤ çš„æŠ½è±¡å±¤

**è¨˜ä½ï¼šå¥½çš„è¨­è¨ˆè®“æ·»åŠ æ–° LLM æä¾›è€…è®Šæˆåªéœ€è¦å¯¦ç¾ä¸€å€‹ä»‹é¢çš„ç°¡å–®å·¥ä½œã€‚**