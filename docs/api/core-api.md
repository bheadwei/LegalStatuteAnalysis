# æ ¸å¿ƒå¼•æ“ API æ–‡æª” - LegalStatuteAnalysis_V1

> **æ–‡ä»¶ç‰ˆæœ¬**ï¼š1.0
> **æœ€å¾Œæ›´æ–°**ï¼š2025-09-23
> **æ¨¡çµ„è·¯å¾‘**ï¼š`src.main.python.core`
> **ç‹€æ…‹**ï¼šæ´»èº

---

## ğŸ“‹ æ¦‚è¦½

æ ¸å¿ƒå¼•æ“æ¨¡çµ„å¯¦ç¾ LLM é©…å‹•çš„æ™ºèƒ½åˆ†æåŠŸèƒ½ï¼Œæ¡ç”¨çµ±ä¸€ä»‹é¢è¨­è¨ˆï¼Œæ¶ˆé™¤ä¸åŒ LLM æä¾›è€…é–“çš„ç‰¹æ®Šæƒ…æ³è™•ç†ã€‚éµå¾ª Linus "å¯¦ç”¨ä¸»ç¾©" åŸå‰‡ï¼Œå°ˆæ³¨æ–¼è§£æ±ºå¯¦éš›çš„æ³•æ¢åŒ¹é…å•é¡Œã€‚

---

## ğŸ§  ä¸»è¦çµ„ä»¶

### EmbeddingMatcher - æ ¸å¿ƒåˆ†æå¼•æ“

**ç”¨é€”ï¼š** çµ±ä¸€çš„ LLM åˆ†æä»‹é¢ï¼Œæ”¯æ´å¤šç¨® LLM æä¾›è€…

```python
from src.main.python.core import EmbeddingMatcher
from src.main.python.models import SystemConfig, LLMProvider

class EmbeddingMatcher:
    """
    æ ¸å¿ƒåˆ†æå¼•æ“ - çµ±ä¸€ä»‹é¢ï¼Œç„¡ç‰¹æ®Šæƒ…æ³

    è¨­è¨ˆåŸå‰‡ï¼š
    - å–®ä¸€è·è²¬ï¼šåªè² è²¬å•é¡Œèˆ‡æ³•æ¢çš„åŒ¹é…åˆ†æ
    - ç„¡ç‰¹æ®Šæƒ…æ³ï¼šæ‰€æœ‰ LLM æä¾›è€…ä½¿ç”¨ç›¸åŒé‚è¼¯
    - ç°¡æ½”ä»‹é¢ï¼šæä¾›æœ€å°‘ä½†è¶³å¤ çš„æ–¹æ³•
    """

    def __init__(self, config: SystemConfig):
        """åˆå§‹åŒ–åˆ†æå¼•æ“"""

    async def initialize_llm(self, provider_name: str) -> None:
        """åˆå§‹åŒ–æŒ‡å®šçš„ LLM æä¾›è€…"""

    async def analyze_question(self, question: ExamQuestion) -> Optional[AnalysisResult]:
        """åˆ†æå–®ä¸€å•é¡Œ - æ ¸å¿ƒæ–¹æ³•"""

    async def analyze_questions(self,
                               questions: List[ExamQuestion],
                               limit: Optional[int] = None) -> List[AnalysisResult]:
        """æ‰¹é‡åˆ†æå•é¡Œ"""

    def generate_report(self, results: List[AnalysisResult]) -> Dict[str, Any]:
        """ç”¢ç”Ÿåˆ†æå ±å‘Š"""
```

**åˆå§‹åŒ–ç¯„ä¾‹ï¼š**

```python
from src.main.python.models import SystemConfig
from src.main.python.core import EmbeddingMatcher

# è¼‰å…¥é…ç½®
config = SystemConfig.load_from_file("src/main/resources/config/law_config.json")

# åˆå§‹åŒ–åˆ†æå™¨
matcher = EmbeddingMatcher(config)

# è¨­å®š LLM æä¾›è€…
await matcher.initialize_llm("openai")  # æˆ– "claude", "simulation"
```

---

### æ ¸å¿ƒåˆ†ææ–¹æ³•

#### analyze_question() - å–®ä¸€å•é¡Œåˆ†æ

**ç°½åï¼š**
```python
async def analyze_question(self, question: ExamQuestion) -> Optional[AnalysisResult]:
```

**åƒæ•¸ï¼š**
- `question: ExamQuestion` - å¾…åˆ†æçš„è€ƒè©¦é¡Œç›®

**å›å‚³ï¼š**
- `Optional[AnalysisResult]` - åˆ†æçµæœï¼Œå¤±æ•—æ™‚å›å‚³ None

**ä½¿ç”¨ç¯„ä¾‹ï¼š**

```python
from src.main.python.models import ExamQuestion, QuestionType

# å»ºç«‹å•é¡Œ
question = ExamQuestion(
    question_id="Q001",
    content="ä¸å‹•ç”¢ç¶“ç´€æ¥­è€…æ‡‰æ–¼ç‡Ÿæ¥­è™•æ‰€æ˜é¡¯è™•æ­ç¤ºå“ªäº›è³‡è¨Šï¼Ÿ",
    question_type=QuestionType.MULTIPLE_CHOICE,
    options=["A. ç‡Ÿæ¥­åŸ·ç…§", "B. ç¶“ç´€äººè­‰æ›¸", "C. æ”¶è²»æ¨™æº–", "D. ä»¥ä¸Šçš†æ˜¯"]
)

# åˆ†æå•é¡Œ
result = await matcher.analyze_question(question)

if result:
    print(f"ä¿¡å¿ƒåº¦: {result.confidence:.2f}")
    print(f"ä¸»è¦æ³•æ¢: {result.primary_article}")
    print(f"åŒ¹é…æ³•æ¢: {result.matched_articles}")
```

#### analyze_questions() - æ‰¹é‡åˆ†æ

**ç°½åï¼š**
```python
async def analyze_questions(self,
                           questions: List[ExamQuestion],
                           limit: Optional[int] = None) -> List[AnalysisResult]:
```

**åƒæ•¸ï¼š**
- `questions: List[ExamQuestion]` - å•é¡Œåˆ—è¡¨
- `limit: Optional[int]` - é™åˆ¶è™•ç†æ•¸é‡ï¼ŒNone è¡¨ç¤ºè™•ç†å…¨éƒ¨

**å›å‚³ï¼š**
- `List[AnalysisResult]` - åˆ†æçµæœåˆ—è¡¨

**ä½¿ç”¨ç¯„ä¾‹ï¼š**

```python
# è¼‰å…¥å•é¡Œ
questions = ExamQuestionLoader.load_from_json("results/exam_113_complete.json")

# æ‰¹é‡åˆ†æï¼ˆé™åˆ¶å‰10é¡Œï¼‰
results = await matcher.analyze_questions(questions, limit=10)

print(f"æˆåŠŸåˆ†æ {len(results)} é“é¡Œç›®")
```

#### generate_report() - ç”¢ç”Ÿå ±å‘Š

**ç°½åï¼š**
```python
def generate_report(self, results: List[AnalysisResult]) -> Dict[str, Any]:
```

**åƒæ•¸ï¼š**
- `results: List[AnalysisResult]` - åˆ†æçµæœåˆ—è¡¨

**å›å‚³ï¼š**
- `Dict[str, Any]` - çµæ§‹åŒ–å ±å‘Šè³‡æ–™

**å ±å‘Šæ ¼å¼ï¼š**

```python
{
    "metadata": {
        "total_questions": 27,
        "successful_analyses": 20,
        "average_confidence": 0.756,
        "success_rate": 0.741,
        "processing_time_total": 45.2,
        "llm_provider": "openai",
        "model": "gpt-4o-mini"
    },
    "question_mappings": [
        {
            "question_id": "Q001",
            "confidence": 0.85,
            "confidence_level": "é«˜",
            "primary_article": "REA-ACT-13",
            "matched_articles": ["REA-ACT-13", "REA-ACT-14"],
            "processing_time": 2.3
        }
        # ... æ›´å¤šçµæœ
    ],
    "statistics": {
        "confidence_distribution": {
            "æ¥µé«˜": 5,
            "é«˜": 8,
            "ä¸­ç­‰": 4,
            "ä½": 2,
            "æ¥µä½": 1
        },
        "most_referenced_articles": [
            {"article_id": "REA-ACT-13", "count": 12},
            {"article_id": "REA-ACT-14", "count": 8}
        ]
    }
}
```

---

### GeminiEmbeddingMatcher - Gemini ç‰¹åŒ–ç‰ˆæœ¬

**ç”¨é€”ï¼š** å°ˆé–€ç”¨æ–¼ Google Gemini API çš„ embedding åŒ¹é…å™¨

```python
from src.main.python.core import GeminiEmbeddingMatcher

class GeminiEmbeddingMatcher:
    """
    Gemini ç‰¹åŒ–åŒ¹é…å™¨ - ä½¿ç”¨ embedding ç›¸ä¼¼åº¦æ¯”å°

    ç‰¹è‰²ï¼š
    - ä½¿ç”¨ Gemini embedding-001 æ¨¡å‹
    - æ”¯æ´å‘é‡åŒ–ç›¸ä¼¼åº¦è¨ˆç®—
    - æä¾›è©³ç´°çš„åŒ¹é…åˆ†æ•¸
    """

    def __init__(self, gemini_api_key: str, embedding_model: str = "models/embedding-001"):
        """åˆå§‹åŒ– Gemini åŒ¹é…å™¨"""

    def load_law_articles(self, articles_path: str) -> None:
        """è¼‰å…¥æ³•æ¢è³‡æ–™"""

    async def generate_embeddings(self, texts: List[str]) -> np.ndarray:
        """ç”¢ç”Ÿæ–‡å­—åµŒå…¥å‘é‡"""

    async def match_question_to_articles(self, question_content: str) -> MatchResult:
        """åŒ¹é…å•é¡Œåˆ°æ³•æ¢"""

    async def match_options_to_articles(self, question_id: str, options: Dict[str, str]) -> List[OptionMatchResult]:
        """åŒ¹é…é¸é …åˆ°æ³•æ¢"""
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**

```python
import os
from src.main.python.core import GeminiEmbeddingMatcher

# åˆå§‹åŒ– Gemini åŒ¹é…å™¨
api_key = os.getenv("GEMINI_API_KEY")
gemini_matcher = GeminiEmbeddingMatcher(api_key)

# è¼‰å…¥æ³•æ¢è³‡æ–™
gemini_matcher.load_law_articles("results/law_articles.csv")

# åŒ¹é…å•é¡Œ
question_content = "ä¸å‹•ç”¢ç¶“ç´€æ¥­è€…æ‡‰æ–¼ç‡Ÿæ¥­è™•æ‰€æ˜é¡¯è™•æ­ç¤ºå“ªäº›è³‡è¨Šï¼Ÿ"
match_result = await gemini_matcher.match_question_to_articles(question_content)

print(f"åŒ¹é…åˆ° {len(match_result.matched_articles)} æ¢ç›¸é—œæ³•æ¢")
for article in match_result.matched_articles:
    print(f"æ³•æ¢: {article['article_id']}, ç›¸ä¼¼åº¦: {article['similarity']:.3f}")
```

---

## ğŸ”§ LLM æä¾›è€…ä»‹é¢

### çµ±ä¸€ LLM ä»‹é¢

æ‰€æœ‰ LLM æä¾›è€…éƒ½å¯¦ç¾ç›¸åŒçš„ä»‹é¢ï¼Œç¢ºä¿ç„¡ç‰¹æ®Šæƒ…æ³è™•ç†ï¼š

```python
from abc import ABC, abstractmethod

class LLMProvider(ABC):
    """LLM æä¾›è€…çµ±ä¸€ä»‹é¢"""

    @abstractmethod
    async def analyze(self, prompt: str) -> str:
        """åˆ†ææ–¹æ³• - æ‰€æœ‰æä¾›è€…å¿…é ˆå¯¦ç¾"""
        pass

    @abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """å–å¾—æ¨¡å‹è³‡è¨Š"""
        pass
```

### å…·é«”å¯¦ç¾

#### OpenAIProvider

```python
class OpenAIProvider(LLMProvider):
    """OpenAI GPT ç³»åˆ—æä¾›è€…"""

    def __init__(self, config: LLMConfig):
        self.client = openai.AsyncOpenAI(api_key=os.getenv(config.api_key_env))
        self.model = config.model
        self.temperature = config.temperature
        self.max_tokens = config.max_tokens

    async def analyze(self, prompt: str) -> str:
        """ä½¿ç”¨ GPT æ¨¡å‹åˆ†æ"""
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        return response.choices[0].message.content
```

#### ClaudeProvider

```python
class ClaudeProvider(LLMProvider):
    """Anthropic Claude æä¾›è€…"""

    async def analyze(self, prompt: str) -> str:
        """ä½¿ç”¨ Claude æ¨¡å‹åˆ†æ"""
        # Claude API å¯¦ç¾
        pass
```

#### SimulationProvider

```python
class SimulationProvider(LLMProvider):
    """æ¨¡æ“¬æä¾›è€… - ç”¨æ–¼æ¸¬è©¦å’Œé–‹ç™¼"""

    async def analyze(self, prompt: str) -> str:
        """å›å‚³æ¨¡æ“¬çš„åˆ†æçµæœ"""
        return self._generate_mock_response(prompt)

    def _generate_mock_response(self, prompt: str) -> str:
        """ç”¢ç”Ÿæ¨¡æ“¬å›æ‡‰ - ç”¨æ–¼æ¸¬è©¦"""
        return json.dumps({
            "confidence": 0.75,
            "matched_articles": ["REA-ACT-13"],
            "reasoning": "æ¨¡æ“¬åˆ†æçµæœï¼šæ ¹æ“šé¡Œç›®å…§å®¹åˆ¤æ–·..."
        }, ensure_ascii=False)
```

---

## ğŸ“Š åˆ†ææµç¨‹

### å®Œæ•´åˆ†æç®¡é“

```mermaid
graph TD
    A[ExamQuestion] --> B[EmbeddingMatcher]
    B --> C[LLM Provider]
    C --> D[Raw Response]
    D --> E[Response Parser]
    E --> F[AnalysisResult]
    F --> G[Report Generator]
    G --> H[Final Report]

    I[Law Articles] --> B
    J[System Config] --> B
```

### å…§éƒ¨è™•ç†é‚è¼¯

```python
async def analyze_question(self, question: ExamQuestion) -> Optional[AnalysisResult]:
    """åˆ†æå•é¡Œçš„å…§éƒ¨æµç¨‹"""

    # 1. å»ºæ§‹æç¤ºè©
    prompt = self._build_analysis_prompt(question)

    # 2. èª¿ç”¨ LLM
    try:
        raw_response = await self.llm_provider.analyze(prompt)
    except Exception as e:
        logger.error(f"LLM analysis failed: {e}")
        return None

    # 3. è§£æå›æ‡‰
    try:
        analysis_data = self._parse_llm_response(raw_response)
    except Exception as e:
        logger.error(f"Response parsing failed: {e}")
        return None

    # 4. å»ºæ§‹çµæœ
    result = AnalysisResult(
        question_id=question.question_id,
        confidence=analysis_data["confidence"],
        confidence_level=ConfidenceLevel.from_score(analysis_data["confidence"]),
        matched_articles=analysis_data["matched_articles"],
        primary_article=analysis_data.get("primary_article"),
        reasoning=analysis_data.get("reasoning"),
        llm_provider=self.current_provider,
        raw_response=raw_response
    )

    return result
```

---

## âš™ï¸ é…ç½®èˆ‡åˆå§‹åŒ–

### ç³»çµ±é…ç½®ç¯„ä¾‹

```json
{
  "llm_config": {
    "default_provider": "openai",
    "fallback_provider": "simulation",
    "providers": {
      "openai": {
        "model": "gpt-4o-mini",
        "temperature": 0,
        "max_tokens": 4000,
        "api_key_env": "OPENAI_API_KEY"
      },
      "claude": {
        "model": "claude-3-haiku-20240307",
        "temperature": 0,
        "max_tokens": 4000,
        "api_key_env": "ANTHROPIC_API_KEY"
      },
      "simulation": {
        "model": "sim-v1",
        "description": "æœ¬åœ°æ¨¡æ“¬å™¨ï¼Œç”¨æ–¼æ¸¬è©¦"
      }
    }
  }
}
```

### åˆå§‹åŒ–æœ€ä½³å¯¦è¸

```python
async def initialize_analysis_system():
    """åˆå§‹åŒ–åˆ†æç³»çµ±çš„æ¨è–¦æ–¹å¼"""

    # 1. è¼‰å…¥é…ç½®
    config = SystemConfig.load_from_file("src/main/resources/config/law_config.json")

    # 2. å»ºç«‹åˆ†æå™¨
    matcher = EmbeddingMatcher(config)

    # 3. å˜—è©¦åˆå§‹åŒ–ä¸»è¦æä¾›è€…
    try:
        await matcher.initialize_llm(config.llm_config["default_provider"])
        logger.info(f"âœ… æˆåŠŸåˆå§‹åŒ– {config.llm_config['default_provider']} æä¾›è€…")
    except Exception as e:
        logger.warning(f"ä¸»è¦æä¾›è€…åˆå§‹åŒ–å¤±æ•—: {e}")

        # 4. é™ç´šåˆ°å¾Œå‚™æä¾›è€…
        fallback_provider = config.llm_config["fallback_provider"]
        await matcher.initialize_llm(fallback_provider)
        logger.info(f"âœ… ä½¿ç”¨å¾Œå‚™æä¾›è€…: {fallback_provider}")

    return matcher
```

---

## ğŸš¨ éŒ¯èª¤è™•ç†

### éŒ¯èª¤é¡å‹

```python
class AnalysisError(Exception):
    """åˆ†æéç¨‹ä¸­çš„éŒ¯èª¤"""
    pass

class LLMProviderError(AnalysisError):
    """LLM æä¾›è€…éŒ¯èª¤"""
    pass

class ConfigurationError(AnalysisError):
    """é…ç½®éŒ¯èª¤"""
    pass
```

### éŒ¯èª¤è™•ç†ç­–ç•¥

```python
async def safe_analyze_question(matcher: EmbeddingMatcher,
                               question: ExamQuestion) -> Optional[AnalysisResult]:
    """å®‰å…¨çš„å•é¡Œåˆ†æ - Linus å¼éŒ¯èª¤è™•ç†ï¼šå¿«é€Ÿå¤±æ•—ï¼Œè©³ç´°è¨˜éŒ„"""

    try:
        return await matcher.analyze_question(question)
    except LLMProviderError as e:
        logger.error(f"LLM æä¾›è€…éŒ¯èª¤ - Question {question.question_id}: {e}")
        return None
    except ConfigurationError as e:
        logger.error(f"é…ç½®éŒ¯èª¤ - Question {question.question_id}: {e}")
        return None
    except Exception as e:
        logger.error(f"æœªçŸ¥éŒ¯èª¤ - Question {question.question_id}: {e}")
        return None
```

---

## ğŸ“ˆ æ€§èƒ½è€ƒé‡

### æ‰¹é‡è™•ç†å„ªåŒ–

```python
async def analyze_questions_optimized(self,
                                    questions: List[ExamQuestion],
                                    batch_size: int = 5) -> List[AnalysisResult]:
    """å„ªåŒ–çš„æ‰¹é‡åˆ†æ - ç°¡å–®çš„ä¸¦è¡Œè™•ç†"""

    results = []

    for i in range(0, len(questions), batch_size):
        batch = questions[i:i + batch_size]

        # ä¸¦è¡Œè™•ç†æ‰¹æ¬¡
        batch_tasks = [self.analyze_question(q) for q in batch]
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

        # éæ¿¾æœ‰æ•ˆçµæœ
        valid_results = [r for r in batch_results if isinstance(r, AnalysisResult)]
        results.extend(valid_results)

        # ç°¡å–®çš„é€Ÿç‡é™åˆ¶
        if i + batch_size < len(questions):
            await asyncio.sleep(1)  # é˜²æ­¢ API é€Ÿç‡é™åˆ¶

    return results
```

---

## ğŸ¯ ä½¿ç”¨æ¨¡å¼ç¸½çµ

### åŸºæœ¬ä½¿ç”¨æµç¨‹

```python
# 1. åˆå§‹åŒ–
config = SystemConfig.load_from_file("config/law_config.json")
matcher = EmbeddingMatcher(config)
await matcher.initialize_llm("openai")

# 2. åˆ†æå–®ä¸€å•é¡Œ
question = ExamQuestion(...)
result = await matcher.analyze_question(question)

# 3. æ‰¹é‡åˆ†æ
questions = ExamQuestionLoader.load_from_json("exam_data.json")
results = await matcher.analyze_questions(questions, limit=10)

# 4. ç”¢ç”Ÿå ±å‘Š
report = matcher.generate_report(results)
print(f"å¹³å‡ä¿¡å¿ƒåº¦: {report['metadata']['average_confidence']:.3f}")
```

### è¨­è¨ˆåŸå‰‡é«”ç¾

**Linus çš„å¯¦ç”¨ä¸»ç¾©ï¼š**
- âœ… è§£æ±ºå¯¦éš›å•é¡Œï¼šæ³•æ¢èˆ‡è€ƒé¡Œçš„ç²¾ç¢ºåŒ¹é…
- âœ… é¿å…éåº¦è¨­è¨ˆï¼šçµ±ä¸€çš„ LLM ä»‹é¢ï¼Œç„¡è¤‡é›œçš„æŠ½è±¡å±¤
- âœ… ç°¡å–®å¯é ï¼šéŒ¯èª¤å¿«é€Ÿå¤±æ•—ï¼Œä¸å˜—è©¦è¤‡é›œæ¢å¾©

**å¥½å“å‘³çš„é«”ç¾ï¼š**
- âœ… æ¶ˆé™¤ç‰¹æ®Šæƒ…æ³ï¼šæ‰€æœ‰ LLM æä¾›è€…ä½¿ç”¨ç›¸åŒè™•ç†é‚è¼¯
- âœ… å–®ä¸€è·è²¬ï¼šæ¯å€‹é¡åˆ¥éƒ½æœ‰æ˜ç¢ºã€å–®ä¸€çš„è²¬ä»»
- âœ… æ¸…æ™°ä»‹é¢ï¼šæ–¹æ³•ç°½åç›´è§€ï¼Œåƒæ•¸æ•¸é‡åˆç†

---

## ğŸ“š ç›¸é—œæ–‡æª”

- [è³‡æ–™æ¨¡å‹ API](./models-api.md) - è³‡æ–™çµæ§‹èªªæ˜
- [é–‹ç™¼æŒ‡å—](../dev/development-guide.md) - é–‹ç™¼åŸå‰‡å’Œæµç¨‹
- [å¿«é€Ÿå…¥é–€](../user/quick-start.md) - å¯¦éš›ä½¿ç”¨ç¯„ä¾‹