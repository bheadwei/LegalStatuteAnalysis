# é€²éšä½¿ç”¨æŒ‡å— - LegalStatuteAnalysis_V1

> **æ–‡ä»¶ç‰ˆæœ¬**ï¼š1.0
> **æœ€å¾Œæ›´æ–°**ï¼š2025-09-23
> **é©ç”¨å°è±¡**ï¼šæœ‰ç¶“é©—çš„ä½¿ç”¨è€…ã€é–‹ç™¼è€…
> **å‰ç½®æ¢ä»¶**ï¼šå·²å®Œæˆ[å¿«é€Ÿå…¥é–€æŒ‡å—](./quick-start.md)

---

## ğŸ¯ æ¦‚è¦½

æœ¬æŒ‡å—æ¶µè“‹ LegalStatuteAnalysis_V1 çš„é€²éšåŠŸèƒ½ï¼ŒåŒ…æ‹¬è‡ªè¨‚é…ç½®ã€æ‰¹é‡è™•ç†å„ªåŒ–ã€å¤š LLM ç­–ç•¥ã€æ€§èƒ½èª¿å„ªå’Œæ“´å±•åŠŸèƒ½ã€‚éµå¾ª Linus "å¯¦ç”¨ä¸»ç¾©" åŸå‰‡ï¼Œå°ˆæ³¨æ–¼è§£æ±ºå¯¦éš›çš„è¤‡é›œéœ€æ±‚ã€‚

---

## âš™ï¸ é€²éšé…ç½®

### ç³»çµ±é…ç½®æ·±åº¦è‡ªè¨‚

**é…ç½®æª”æ¡ˆä½ç½®ï¼š** `src/main/resources/config/law_config.json`

```json
{
  "version": "3.0",
  "description": "æ³•è¦èˆ‡è€ƒé¡Œè§£æè¨­å®šæª” - LLM é©…å‹•ç‰ˆæœ¬",

  "law_definitions": {
    "ä¸å‹•ç”¢ç¶“ç´€æ¥­ç®¡ç†æ¢ä¾‹.md": {
      "law_code": "REA-ACT",
      "law_name": "ä¸å‹•ç”¢ç¶“ç´€æ¥­ç®¡ç†æ¢ä¾‹",
      "revision_date_roc": "æ°‘åœ‹ 110 å¹´ 01 æœˆ 27 æ—¥",
      "category": "ä¸å‹•ç”¢ç¶“ç´€",
      "authority": "å…§æ”¿éƒ¨",
      "priority": 1,                    // æ–°å¢ï¼šæ³•æ¢å„ªå…ˆç´š
      "enable_fuzzy_match": true        // æ–°å¢ï¼šå•Ÿç”¨æ¨¡ç³ŠåŒ¹é…
    }
  },

  "exam_sets": {
    "custom_exam_2024": {               // è‡ªè¨‚è€ƒè©¦é›†
      "name": "2024å¹´è‡ªè¨‚æ³•è¦è€ƒè©¦",
      "year": 2024,
      "question_file": "data/custom_questions.json",
      "parser_type": "llm",
      "batch_size": 10,                 // æ–°å¢ï¼šæ‰¹æ¬¡è™•ç†å¤§å°
      "confidence_threshold": 0.7       // æ–°å¢ï¼šä¿¡å¿ƒåº¦é–¾å€¼
    }
  },

  "llm_config": {
    "default_provider": "openai",
    "fallback_provider": "simulation",
    "timeout_seconds": 30,              // æ–°å¢ï¼šè¶…æ™‚è¨­å®š
    "max_concurrent": 5,                // æ–°å¢ï¼šæœ€å¤§ä¸¦ç™¼æ•¸
    "retry_attempts": 3,                // æ–°å¢ï¼šé‡è©¦æ¬¡æ•¸

    "providers": {
      "openai": {
        "model": "gpt-4o-mini",
        "temperature": 0,
        "max_tokens": 4000,
        "api_key_env": "OPENAI_API_KEY",
        "rate_limit_rpm": 500,          // æ–°å¢ï¼šé€Ÿç‡é™åˆ¶
        "cost_per_1k_tokens": 0.00015   // æ–°å¢ï¼šæˆæœ¬è¿½è¹¤
      },

      "custom_local": {                 // æ–°å¢ï¼šè‡ªè¨‚æœ¬åœ°æ¨¡å‹
        "model": "local-llama-7b",
        "base_url": "http://localhost:8080",
        "temperature": 0.1,
        "max_tokens": 2000,
        "custom_headers": {
          "X-Custom-Auth": "your-token"
        }
      }
    }
  },

  "analysis_settings": {                // æ–°å¢ï¼šåˆ†æè¨­å®šå€å¡Š
    "enable_detailed_reasoning": true,
    "include_confidence_breakdown": true,
    "save_raw_responses": false,
    "enable_parallel_processing": true
  }
}
```

### ç’°å¢ƒè®Šæ•¸é€²éšé…ç½®

**å»ºç«‹ `.env.production` æ–‡ä»¶ï¼š**
```env
# ç”Ÿç”¢ç’°å¢ƒé…ç½®
OPENAI_API_KEY=prod_openai_key
ANTHROPIC_API_KEY=prod_claude_key
GEMINI_API_KEY=prod_gemini_key

# æ€§èƒ½èª¿å„ª
MAX_CONCURRENT_REQUESTS=10
REQUEST_TIMEOUT=60
RETRY_DELAY=2

# æ—¥èªŒè¨­å®š
LOG_LEVEL=INFO
LOG_FILE=logs/production.log

# å¿«å–è¨­å®š
ENABLE_CACHE=true
CACHE_TTL=3600
CACHE_DIR=tmp/cache

# æˆæœ¬æ§åˆ¶
MONTHLY_TOKEN_LIMIT=1000000
COST_ALERT_THRESHOLD=50.00
```

**è¼‰å…¥ç‰¹å®šç’°å¢ƒé…ç½®ï¼š**
```bash
# è¼‰å…¥ç”Ÿç”¢ç’°å¢ƒé…ç½®
export ENV=production
python tools/scripts/run_core_embedding.py --env production
```

---

## ğŸš€ æ‰¹é‡è™•ç†å„ªåŒ–

### å¤§è¦æ¨¡åˆ†æç­–ç•¥

**1. æ™ºèƒ½æ‰¹æ¬¡è™•ç†**

```python
# tools/scripts/advanced_batch_analysis.py
import asyncio
import time
from typing import List, Optional
from src.main.python.models import ExamQuestion, SystemConfig
from src.main.python.core import EmbeddingMatcher

class AdvancedBatchProcessor:
    """
    é€²éšæ‰¹é‡è™•ç†å™¨ - Linus å¼å¯¦ç”¨ä¸»ç¾©

    ç‰¹è‰²ï¼š
    - å‹•æ…‹æ‰¹æ¬¡å¤§å°èª¿æ•´
    - æ™ºèƒ½éŒ¯èª¤æ¢å¾©
    - é€²åº¦è¿½è¹¤å’Œçµ±è¨ˆ
    - æˆæœ¬æ§åˆ¶
    """

    def __init__(self, config: SystemConfig):
        self.config = config
        self.matcher = EmbeddingMatcher(config)
        self.stats = {
            "processed": 0,
            "successful": 0,
            "failed": 0,
            "total_cost": 0.0,
            "start_time": None
        }

    async def process_large_dataset(self,
                                   questions: List[ExamQuestion],
                                   initial_batch_size: int = 5,
                                   max_batch_size: int = 20) -> List[AnalysisResult]:
        """æ™ºèƒ½æ‰¹é‡è™•ç†å¤§å‹è³‡æ–™é›†"""

        self.stats["start_time"] = time.time()
        results = []
        batch_size = initial_batch_size

        # åˆå§‹åŒ– LLM
        await self.matcher.initialize_llm(self.config.llm_config["default_provider"])

        for i in range(0, len(questions), batch_size):
            batch = questions[i:i + batch_size]

            # è™•ç†æ‰¹æ¬¡
            batch_start_time = time.time()
            batch_results = await self._process_batch(batch)
            batch_duration = time.time() - batch_start_time

            # å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°
            batch_size = self._adjust_batch_size(
                batch_size, batch_duration, len(batch_results), len(batch)
            )
            batch_size = min(batch_size, max_batch_size)

            results.extend(batch_results)
            self._update_stats(batch_results)

            # é€²åº¦å ±å‘Š
            progress = (i + len(batch)) / len(questions) * 100
            self._print_progress(progress, batch_duration, batch_size)

            # æˆæœ¬æ§åˆ¶æª¢æŸ¥
            if self._check_cost_limit():
                print("âš ï¸ é”åˆ°æˆæœ¬é™åˆ¶ï¼Œåœæ­¢è™•ç†")
                break

            # é©ç•¶çš„å»¶é²é¿å…é€Ÿç‡é™åˆ¶
            if i + batch_size < len(questions):
                await asyncio.sleep(self._calculate_delay(batch_duration))

        return results

    def _adjust_batch_size(self, current_size: int, duration: float,
                          successful: int, total: int) -> int:
        """æ ¹æ“šæ€§èƒ½å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°"""
        success_rate = successful / total if total > 0 else 0
        avg_time_per_item = duration / total if total > 0 else float('inf')

        # å¦‚æœæˆåŠŸç‡é«˜ä¸”è™•ç†å¿«é€Ÿï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°
        if success_rate > 0.9 and avg_time_per_item < 2.0:
            return min(current_size + 2, 20)

        # å¦‚æœæˆåŠŸç‡ä½æˆ–è™•ç†æ…¢ï¼Œæ¸›å°‘æ‰¹æ¬¡å¤§å°
        elif success_rate < 0.7 or avg_time_per_item > 5.0:
            return max(current_size - 1, 1)

        return current_size
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```bash
# å¤§è¦æ¨¡æ‰¹é‡åˆ†æ
python tools/scripts/advanced_batch_analysis.py \
    --input data/large_exam_set.json \
    --provider openai \
    --initial-batch-size 5 \
    --max-batch-size 20 \
    --cost-limit 10.0
```

### åˆ†æ•£å¼è™•ç†

**2. å¤šé€²ç¨‹ä¸¦è¡Œè™•ç†**

```python
# tools/scripts/distributed_analysis.py
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
import json

def process_chunk(chunk_data):
    """è™•ç†è³‡æ–™å¡Šçš„å·¥ä½œå‡½å¼"""
    questions, config_path, provider = chunk_data

    # åœ¨æ¯å€‹é€²ç¨‹ä¸­åˆå§‹åŒ–
    config = SystemConfig.load_from_file(config_path)
    matcher = EmbeddingMatcher(config)

    # è™•ç†é€™å€‹å¡Š
    results = []
    for question in questions:
        result = asyncio.run(matcher.analyze_question(question))
        if result:
            results.append(result)

    return results

def distributed_analysis(questions: List[ExamQuestion],
                        num_processes: int = None) -> List[AnalysisResult]:
    """åˆ†æ•£å¼åˆ†æè™•ç†"""

    if num_processes is None:
        num_processes = min(mp.cpu_count(), 4)  # é™åˆ¶æœ€å¤§é€²ç¨‹æ•¸

    # åˆ†å‰²è³‡æ–™
    chunk_size = len(questions) // num_processes
    chunks = [questions[i:i + chunk_size]
              for i in range(0, len(questions), chunk_size)]

    # æº–å‚™æ¯å€‹å¡Šçš„è³‡æ–™
    chunk_data = [(chunk, "config/law_config.json", "openai")
                  for chunk in chunks]

    # ä¸¦è¡Œè™•ç†
    with ProcessPoolExecutor(max_workers=num_processes) as executor:
        chunk_results = list(executor.map(process_chunk, chunk_data))

    # åˆä½µçµæœ
    all_results = []
    for results in chunk_results:
        all_results.extend(results)

    return all_results
```

---

## ğŸ¤– å¤š LLM ç­–ç•¥

### æ™ºèƒ½æä¾›è€…é¸æ“‡

**3. è‡ªé©æ‡‰ LLM è·¯ç”±**

```python
class AdaptiveLLMRouter:
    """
    è‡ªé©æ‡‰ LLM è·¯ç”±å™¨ - æ ¹æ“šå•é¡Œç‰¹å¾µé¸æ“‡æœ€ä½³æä¾›è€…

    ç­–ç•¥ï¼š
    - ç°¡å–®å•é¡Œ â†’ å¿«é€Ÿä¾¿å®œçš„æ¨¡å‹
    - è¤‡é›œå•é¡Œ â†’ é«˜å“è³ªæ¨¡å‹
    - å¤±æ•—è‡ªå‹•é™ç´š
    """

    def __init__(self, config: SystemConfig):
        self.config = config
        self.providers = {}
        self.performance_history = {}

        # åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨æä¾›è€…
        for provider_name in ["openai", "claude", "gemini", "simulation"]:
            try:
                provider_config = config.get_llm_config(provider_name)
                self.providers[provider_name] = LLMProviderFactory.create_provider(provider_config)
            except Exception as e:
                logger.warning(f"ç„¡æ³•åˆå§‹åŒ–æä¾›è€… {provider_name}: {e}")

    async def analyze_with_best_provider(self, question: ExamQuestion) -> Optional[AnalysisResult]:
        """ä½¿ç”¨æœ€ä½³æä¾›è€…åˆ†æå•é¡Œ"""

        # åˆ†æå•é¡Œè¤‡é›œåº¦
        complexity = self._analyze_question_complexity(question)

        # æ ¹æ“šè¤‡é›œåº¦é¸æ“‡æä¾›è€…ç­–ç•¥
        if complexity < 0.3:
            provider_priority = ["gemini", "openai", "claude", "simulation"]
        elif complexity < 0.7:
            provider_priority = ["openai", "claude", "gemini", "simulation"]
        else:
            provider_priority = ["claude", "openai", "gemini", "simulation"]

        # å˜—è©¦æä¾›è€…
        for provider_name in provider_priority:
            if provider_name not in self.providers:
                continue

            try:
                provider = self.providers[provider_name]
                if not provider.is_available:
                    continue

                result = await provider.analyze(self._build_prompt(question))

                # è¨˜éŒ„æˆåŠŸ
                self._record_success(provider_name, question, result)
                return self._parse_result(result)

            except Exception as e:
                logger.warning(f"æä¾›è€… {provider_name} å¤±æ•—: {e}")
                self._record_failure(provider_name, question, e)
                continue

        return None

    def _analyze_question_complexity(self, question: ExamQuestion) -> float:
        """åˆ†æå•é¡Œè¤‡é›œåº¦ï¼ˆ0.0-1.0ï¼‰"""
        complexity_score = 0.0

        # åŸºæ–¼å…§å®¹é•·åº¦
        content_length = len(question.content)
        complexity_score += min(content_length / 500, 0.3)

        # åŸºæ–¼é¸é …æ•¸é‡
        if question.options:
            complexity_score += min(len(question.options) / 6, 0.2)

        # åŸºæ–¼å•é¡Œé¡å‹
        if question.question_type == QuestionType.ESSAY:
            complexity_score += 0.4
        elif question.question_type == QuestionType.MULTIPLE_CHOICE:
            complexity_score += 0.2

        # åŸºæ–¼é—œéµè©è¤‡é›œåº¦
        complex_keywords = ["ä½†æ›¸", "ä¾‹å¤–", "å¾—", "æ‡‰", "ä¸å¾—", "é™¤"]
        keyword_count = sum(1 for keyword in complex_keywords
                          if keyword in question.content)
        complexity_score += min(keyword_count / 10, 0.1)

        return min(complexity_score, 1.0)
```

### LLM æŠ•ç¥¨æ©Ÿåˆ¶

**4. å¤šæ¨¡å‹å…±è­˜æ±ºç­–**

```python
class EnsembleLLMAnalyzer:
    """
    é›†æˆ LLM åˆ†æå™¨ - å¤šå€‹æ¨¡å‹æŠ•ç¥¨æ±ºå®šæœ€çµ‚çµæœ

    åŸç†ï¼š
    - å¤šå€‹ LLM ç¨ç«‹åˆ†æåŒä¸€å•é¡Œ
    - ç¶œåˆæ‰€æœ‰çµæœç”¢ç”Ÿæœ€çµ‚ç­”æ¡ˆ
    - æä¾›ä¿¡å¿ƒåº¦åŠ æ¬Š
    """

    def __init__(self, providers: List[LLMProvider]):
        self.providers = providers

    async def ensemble_analyze(self, question: ExamQuestion) -> Optional[AnalysisResult]:
        """é›†æˆåˆ†æ - å¤šæ¨¡å‹æŠ•ç¥¨"""

        # ä¸¦è¡Œèª¿ç”¨æ‰€æœ‰æä¾›è€…
        tasks = [provider.analyze(self._build_prompt(question))
                for provider in self.providers if provider.is_available]

        if not tasks:
            return None

        responses = await asyncio.gather(*tasks, return_exceptions=True)

        # è§£ææ‰€æœ‰æœ‰æ•ˆå›æ‡‰
        valid_results = []
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                logger.warning(f"æä¾›è€… {i} å¤±æ•—: {response}")
                continue

            try:
                result = self._parse_response(response)
                if result:
                    valid_results.append(result)
            except Exception as e:
                logger.warning(f"å›æ‡‰è§£æå¤±æ•—: {e}")

        if not valid_results:
            return None

        # æŠ•ç¥¨æ±ºå®šæœ€çµ‚çµæœ
        return self._vote_on_results(valid_results)

    def _vote_on_results(self, results: List[Dict]) -> AnalysisResult:
        """å°å¤šå€‹çµæœé€²è¡ŒæŠ•ç¥¨"""

        # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„æ³•æ¢
        all_articles = []
        article_votes = {}
        confidence_scores = []

        for result in results:
            confidence_scores.append(result.get('confidence', 0.0))

            for article in result.get('matched_articles', []):
                all_articles.append(article)
                article_votes[article] = article_votes.get(article, 0) + 1

        # æ ¹æ“šæŠ•ç¥¨æ•¸æ’åºæ³•æ¢
        sorted_articles = sorted(article_votes.items(),
                               key=lambda x: x[1], reverse=True)

        # è¨ˆç®—ç¶œåˆä¿¡å¿ƒåº¦
        avg_confidence = sum(confidence_scores) / len(confidence_scores)

        # ä¿¡å¿ƒåº¦åŠ æ¬Šï¼šå¦‚æœå¤šå€‹æ¨¡å‹éƒ½çµ¦å‡ºé«˜ä¿¡å¿ƒåº¦ï¼Œæå‡æœ€çµ‚ä¿¡å¿ƒåº¦
        confidence_boost = min(len(results) * 0.1, 0.3)
        final_confidence = min(avg_confidence + confidence_boost, 1.0)

        return AnalysisResult(
            question_id=question.question_id,
            confidence=final_confidence,
            confidence_level=ConfidenceLevel.from_score(final_confidence),
            matched_articles=[article for article, _ in sorted_articles],
            primary_article=sorted_articles[0][0] if sorted_articles else None,
            reasoning=f"é›†æˆåˆ†æçµæœï¼š{len(results)} å€‹æ¨¡å‹æŠ•ç¥¨ï¼Œå¹³å‡ä¿¡å¿ƒåº¦ {avg_confidence:.2f}"
        )
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
# é›†æˆåˆ†æ
providers = [
    LLMProviderFactory.create_provider(config.get_llm_config("openai")),
    LLMProviderFactory.create_provider(config.get_llm_config("claude")),
    LLMProviderFactory.create_provider(config.get_llm_config("gemini"))
]

ensemble = EnsembleLLMAnalyzer(providers)
result = await ensemble.ensemble_analyze(question)
```

---

## ğŸ“Š æ€§èƒ½èª¿å„ªèˆ‡ç›£æ§

### è©³ç´°æ€§èƒ½è¿½è¹¤

**5. æ€§èƒ½ç›£æ§å„€è¡¨æ¿**

```python
class PerformanceMonitor:
    """
    æ€§èƒ½ç›£æ§å™¨ - Linus å¼ç°¡æ½”å¯¦ç”¨

    ç›£æ§æŒ‡æ¨™ï¼š
    - éŸ¿æ‡‰æ™‚é–“çµ±è¨ˆ
    - æˆåŠŸç‡è¿½è¹¤
    - æˆæœ¬åˆ†æ
    - éŒ¯èª¤æ¨¡å¼è­˜åˆ¥
    """

    def __init__(self):
        self.metrics = {
            "requests": [],
            "errors": [],
            "costs": [],
            "start_time": time.time()
        }

    def record_request(self, provider: str, duration: float,
                      success: bool, tokens: int = 0, cost: float = 0.0):
        """è¨˜éŒ„è«‹æ±‚æŒ‡æ¨™"""
        self.metrics["requests"].append({
            "timestamp": time.time(),
            "provider": provider,
            "duration": duration,
            "success": success,
            "tokens": tokens,
            "cost": cost
        })

        if cost > 0:
            self.metrics["costs"].append(cost)

    def generate_report(self) -> Dict[str, Any]:
        """ç”¢ç”Ÿæ€§èƒ½å ±å‘Š"""
        requests = self.metrics["requests"]
        if not requests:
            return {"error": "ç„¡æ€§èƒ½è³‡æ–™"}

        # åŸºæœ¬çµ±è¨ˆ
        total_requests = len(requests)
        successful_requests = sum(1 for r in requests if r["success"])
        success_rate = successful_requests / total_requests

        # éŸ¿æ‡‰æ™‚é–“çµ±è¨ˆ
        durations = [r["duration"] for r in requests]
        avg_duration = sum(durations) / len(durations)
        max_duration = max(durations)
        min_duration = min(durations)

        # æˆæœ¬çµ±è¨ˆ
        total_cost = sum(self.metrics["costs"])
        avg_cost_per_request = total_cost / total_requests if total_requests > 0 else 0

        # æä¾›è€…çµ±è¨ˆ
        provider_stats = {}
        for request in requests:
            provider = request["provider"]
            if provider not in provider_stats:
                provider_stats[provider] = {
                    "requests": 0,
                    "successes": 0,
                    "total_duration": 0
                }

            provider_stats[provider]["requests"] += 1
            if request["success"]:
                provider_stats[provider]["successes"] += 1
            provider_stats[provider]["total_duration"] += request["duration"]

        # è¨ˆç®—æ¯å€‹æä¾›è€…çš„å¹³å‡æ€§èƒ½
        for provider, stats in provider_stats.items():
            stats["success_rate"] = stats["successes"] / stats["requests"]
            stats["avg_duration"] = stats["total_duration"] / stats["requests"]

        return {
            "summary": {
                "total_requests": total_requests,
                "success_rate": success_rate,
                "avg_response_time": avg_duration,
                "max_response_time": max_duration,
                "min_response_time": min_duration,
                "total_cost": total_cost,
                "avg_cost_per_request": avg_cost_per_request
            },
            "provider_breakdown": provider_stats,
            "uptime": time.time() - self.metrics["start_time"]
        }
```

### å¿«å–æ©Ÿåˆ¶

**6. æ™ºèƒ½çµæœå¿«å–**

```python
import hashlib
import pickle
import os

class AnalysisCache:
    """
    åˆ†æçµæœå¿«å– - é¿å…é‡è¤‡è¨ˆç®—ç›¸åŒå•é¡Œ

    ç­–ç•¥ï¼š
    - åŸºæ–¼å•é¡Œå…§å®¹çš„ hash å¿«å–
    - LRU æ·˜æ±°ç­–ç•¥
    - å¯é…ç½®çš„éæœŸæ™‚é–“
    """

    def __init__(self, cache_dir: str = "tmp/cache", max_size: int = 1000):
        self.cache_dir = cache_dir
        self.max_size = max_size
        os.makedirs(cache_dir, exist_ok=True)

    def _get_cache_key(self, question: ExamQuestion, provider: str) -> str:
        """ç”¢ç”Ÿå¿«å–éµå€¼"""
        content = f"{question.content}_{provider}"
        return hashlib.md5(content.encode()).hexdigest()

    def get(self, question: ExamQuestion, provider: str) -> Optional[AnalysisResult]:
        """å¾å¿«å–å–å¾—çµæœ"""
        cache_key = self._get_cache_key(question, provider)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.cache")

        if not os.path.exists(cache_file):
            return None

        # æª¢æŸ¥éæœŸæ™‚é–“ï¼ˆ24å°æ™‚ï¼‰
        if time.time() - os.path.getmtime(cache_file) > 86400:
            os.remove(cache_file)
            return None

        try:
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        except Exception:
            return None

    def set(self, question: ExamQuestion, provider: str, result: AnalysisResult):
        """å„²å­˜çµæœåˆ°å¿«å–"""
        cache_key = self._get_cache_key(question, provider)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.cache")

        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)
        except Exception as e:
            logger.warning(f"å¿«å–å„²å­˜å¤±æ•—: {e}")

    def clear_expired(self):
        """æ¸…ç†éæœŸå¿«å–"""
        for filename in os.listdir(self.cache_dir):
            filepath = os.path.join(self.cache_dir, filename)
            if time.time() - os.path.getmtime(filepath) > 86400:
                os.remove(filepath)
```

---

## ğŸ”§ è‡ªè¨‚æ“´å±•

### è‡ªè¨‚ LLM æä¾›è€…

**7. å»ºç«‹è‡ªè¨‚æä¾›è€…**

```python
class CustomLocalProvider(LLMProvider):
    """
    è‡ªè¨‚æœ¬åœ° LLM æä¾›è€…ç¯„ä¾‹

    é©ç”¨å ´æ™¯ï¼š
    - æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹ï¼ˆOllama, LocalAI ç­‰ï¼‰
    - è‡ªå»ºçš„ API æœå‹™
    - ç‰¹æ®Šçš„æ¨¡å‹èª¿ç”¨é‚è¼¯
    """

    def __init__(self, config: LLMConfig):
        self.config = config
        self.base_url = config.base_url or "http://localhost:11434"
        self.model = config.model

    async def analyze(self, prompt: str) -> str:
        """èª¿ç”¨æœ¬åœ° LLM API"""
        import aiohttp

        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": self.config.temperature,
                "top_p": 0.9,
                "max_tokens": self.config.max_tokens
            }
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                if response.status != 200:
                    raise LLMProviderError(f"æœ¬åœ° API éŒ¯èª¤: {response.status}")

                result = await response.json()
                return result.get("response", "")

    def get_model_info(self) -> Dict[str, Any]:
        return {
            "provider": "custom_local",
            "model": self.model,
            "base_url": self.base_url,
            "version": "1.0"
        }

    @property
    def is_available(self) -> bool:
        """æª¢æŸ¥æœ¬åœ°æœå‹™æ˜¯å¦å¯ç”¨"""
        try:
            import requests
            response = requests.get(f"{self.base_url}/api/version", timeout=5)
            return response.status_code == 200
        except:
            return False

# è¨»å†Šè‡ªè¨‚æä¾›è€…
LLMProviderFactory.register_provider(
    LLMProviderEnum.LOCAL,
    CustomLocalProvider
)
```

### è‡ªè¨‚åˆ†æé‚è¼¯

**8. æ“´å±•åˆ†ææµç¨‹**

```python
class CustomAnalysisProcessor:
    """
    è‡ªè¨‚åˆ†æè™•ç†å™¨ - æ“´å±•æ ¸å¿ƒåˆ†æé‚è¼¯

    ç”¨é€”ï¼š
    - æ·»åŠ é è™•ç†æ­¥é©Ÿ
    - å¯¦ç¾è‡ªè¨‚åŒ¹é…ç®—æ³•
    - æ•´åˆå¤–éƒ¨è³‡æ–™æº
    """

    def __init__(self, matcher: EmbeddingMatcher):
        self.matcher = matcher
        self.external_db = self._init_external_db()

    async def enhanced_analyze(self, question: ExamQuestion) -> Optional[AnalysisResult]:
        """å¢å¼·åˆ†ææµç¨‹"""

        # 1. é è™•ç†å•é¡Œ
        processed_question = self._preprocess_question(question)

        # 2. æª¢æŸ¥å¤–éƒ¨è³‡æ–™åº«
        external_hints = await self._query_external_db(processed_question)

        # 3. åŸ·è¡Œæ ¸å¿ƒåˆ†æ
        core_result = await self.matcher.analyze_question(processed_question)

        if not core_result:
            return None

        # 4. æ•´åˆå¤–éƒ¨æç¤º
        enhanced_result = self._integrate_external_hints(core_result, external_hints)

        # 5. å¾Œè™•ç†çµæœ
        final_result = self._postprocess_result(enhanced_result)

        return final_result

    def _preprocess_question(self, question: ExamQuestion) -> ExamQuestion:
        """é è™•ç†å•é¡Œ - æ¸…ç†å’Œæ¨™æº–åŒ–"""
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        cleaned_content = ' '.join(question.content.split())

        # æ¨™æº–åŒ–è¡“èª
        standardized_content = self._standardize_legal_terms(cleaned_content)

        return ExamQuestion(
            question_id=question.question_id,
            content=standardized_content,
            question_type=question.question_type,
            options=question.options
        )

    def _standardize_legal_terms(self, content: str) -> str:
        """æ¨™æº–åŒ–æ³•å¾‹è¡“èª"""
        term_mapping = {
            "ä¸å‹•ç”¢ä»²ä»‹": "ä¸å‹•ç”¢ç¶“ç´€",
            "æˆ¿ä»²": "ä¸å‹•ç”¢ç¶“ç´€",
            "åœ°æ”¿å£«": "ä¸å‹•ç”¢ä¼°åƒ¹å¸«"
        }

        for old_term, new_term in term_mapping.items():
            content = content.replace(old_term, new_term)

        return content
```

---

## ğŸ“ˆ æˆæœ¬æ§åˆ¶èˆ‡å„ªåŒ–

### æ™ºèƒ½æˆæœ¬ç®¡ç†

**9. æˆæœ¬è¿½è¹¤èˆ‡æ§åˆ¶**

```python
class CostController:
    """
    æˆæœ¬æ§åˆ¶å™¨ - æ™ºèƒ½ç®¡ç† LLM ä½¿ç”¨æˆæœ¬

    åŠŸèƒ½ï¼š
    - å¯¦æ™‚æˆæœ¬è¿½è¹¤
    - é ç®—è­¦å ±
    - è‡ªå‹•é™ç´šç­–ç•¥
    """

    def __init__(self, monthly_budget: float = 100.0):
        self.monthly_budget = monthly_budget
        self.current_month_cost = 0.0
        self.cost_history = []

        # æä¾›è€…æˆæœ¬ç‡ï¼ˆæ¯1K tokensï¼‰
        self.cost_rates = {
            "openai": {"gpt-4o-mini": 0.00015, "gpt-4o": 0.005},
            "claude": {"haiku": 0.00025, "sonnet": 0.003, "opus": 0.015},
            "gemini": {"flash": 0.000075, "pro": 0.0005}
        }

    async def controlled_analyze(self, matcher: EmbeddingMatcher,
                               question: ExamQuestion) -> Optional[AnalysisResult]:
        """æˆæœ¬æ§åˆ¶çš„åˆ†æ"""

        # æª¢æŸ¥é ç®—
        if self.current_month_cost >= self.monthly_budget:
            logger.warning("å·²é”æœˆé ç®—ä¸Šé™ï¼Œåˆ‡æ›åˆ°å…è²»æ¨¡å¼")
            await matcher.initialize_llm("simulation")

        # é ä¼°æˆæœ¬
        estimated_cost = self._estimate_cost(question, matcher.current_provider)

        if self.current_month_cost + estimated_cost > self.monthly_budget * 0.9:
            logger.warning("æ¥è¿‘é ç®—ä¸Šé™ï¼Œåˆ‡æ›åˆ°ä½æˆæœ¬æä¾›è€…")
            await matcher.initialize_llm("gemini")  # åˆ‡æ›åˆ°è¼ƒä¾¿å®œçš„é¸é …

        # åŸ·è¡Œåˆ†æ
        start_time = time.time()
        result = await matcher.analyze_question(question)
        duration = time.time() - start_time

        # è¨˜éŒ„å¯¦éš›æˆæœ¬
        if result:
            actual_cost = self._calculate_actual_cost(question, result, matcher.current_provider)
            self._record_cost(actual_cost, duration)

        return result

    def _estimate_cost(self, question: ExamQuestion, provider: str) -> float:
        """ä¼°ç®—åˆ†ææˆæœ¬"""
        # ç°¡åŒ–çš„ token ä¼°ç®—
        estimated_tokens = len(question.content.split()) * 2  # ç²—ç•¥ä¼°ç®—

        if provider in self.cost_rates:
            provider_rates = self.cost_rates[provider]
            model_rate = list(provider_rates.values())[0]  # ä½¿ç”¨ç¬¬ä¸€å€‹æ¨¡å‹çš„è²»ç‡
            return estimated_tokens * model_rate / 1000

        return 0.0  # å…è²»æä¾›è€…

    def get_cost_summary(self) -> Dict[str, Any]:
        """å–å¾—æˆæœ¬æ‘˜è¦"""
        return {
            "monthly_budget": self.monthly_budget,
            "current_month_cost": self.current_month_cost,
            "remaining_budget": self.monthly_budget - self.current_month_cost,
            "budget_utilization": self.current_month_cost / self.monthly_budget,
            "total_requests": len(self.cost_history),
            "avg_cost_per_request": (
                sum(h["cost"] for h in self.cost_history) / len(self.cost_history)
                if self.cost_history else 0
            )
        }
```

---

## ğŸ” é€²éšæŸ¥è©¢èˆ‡åˆ†æ

### è¤‡é›œæŸ¥è©¢æ”¯æ´

**10. è‡ªè¨‚æŸ¥è©¢ä»‹é¢**

```python
class AdvancedQueryInterface:
    """
    é€²éšæŸ¥è©¢ä»‹é¢ - æ”¯æ´è¤‡é›œçš„åˆ†æéœ€æ±‚

    åŠŸèƒ½ï¼š
    - æ¢ä»¶ç¯©é¸
    - æ‰¹é‡æ¯”è¼ƒ
    - è¶¨å‹¢åˆ†æ
    - è‡ªè¨‚å ±å‘Š
    """

    def __init__(self, results_db: str = "results/analysis_results.db"):
        import sqlite3
        self.conn = sqlite3.connect(results_db)
        self._init_database()

    def _init_database(self):
        """åˆå§‹åŒ–çµæœè³‡æ–™åº«"""
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS analysis_results (
                id INTEGER PRIMARY KEY,
                question_id TEXT,
                question_content TEXT,
                confidence REAL,
                matched_articles TEXT,
                primary_article TEXT,
                llm_provider TEXT,
                analysis_date DATE,
                processing_time REAL
            )
        """)
        self.conn.commit()

    def store_result(self, result: AnalysisResult, question: ExamQuestion):
        """å„²å­˜åˆ†æçµæœ"""
        self.conn.execute("""
            INSERT INTO analysis_results
            (question_id, question_content, confidence, matched_articles,
             primary_article, llm_provider, analysis_date, processing_time)
            VALUES (?, ?, ?, ?, ?, ?, DATE('now'), ?)
        """, (
            result.question_id,
            question.content,
            result.confidence,
            ','.join(result.matched_articles),
            result.primary_article,
            result.llm_provider.value if result.llm_provider else None,
            result.processing_time
        ))
        self.conn.commit()

    def query_by_confidence(self, min_confidence: float = 0.7) -> List[Dict]:
        """æŸ¥è©¢é«˜ä¿¡å¿ƒåº¦çµæœ"""
        cursor = self.conn.execute("""
            SELECT * FROM analysis_results
            WHERE confidence >= ?
            ORDER BY confidence DESC
        """, (min_confidence,))

        columns = [desc[0] for desc in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]

    def analyze_performance_trends(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶¨å‹¢"""
        # æŒ‰æ—¥æœŸçµ±è¨ˆ
        cursor = self.conn.execute("""
            SELECT
                analysis_date,
                COUNT(*) as total_analyses,
                AVG(confidence) as avg_confidence,
                AVG(processing_time) as avg_processing_time,
                llm_provider
            FROM analysis_results
            GROUP BY analysis_date, llm_provider
            ORDER BY analysis_date DESC
        """)

        trends = cursor.fetchall()

        # è¨ˆç®—æ•´é«”çµ±è¨ˆ
        cursor = self.conn.execute("""
            SELECT
                COUNT(*) as total,
                AVG(confidence) as overall_avg_confidence,
                MIN(confidence) as min_confidence,
                MAX(confidence) as max_confidence
            FROM analysis_results
        """)

        overall_stats = cursor.fetchone()

        return {
            "daily_trends": trends,
            "overall_statistics": {
                "total_analyses": overall_stats[0],
                "average_confidence": overall_stats[1],
                "confidence_range": [overall_stats[2], overall_stats[3]]
            }
        }
```

---

## ğŸ¯ æœ€ä½³å¯¦è¸ç¸½çµ

### é€²éšä½¿ç”¨å»ºè­°

**1. ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²**

```bash
# ç”Ÿç”¢ç’°å¢ƒå•Ÿå‹•è…³æœ¬
#!/bin/bash
# production_start.sh

export ENV=production
export LOG_LEVEL=INFO
export MAX_CONCURRENT_REQUESTS=10

# æ¸…ç†å¿«å–
python tools/scripts/cache_cleanup.py

# é ç†±ç³»çµ±
python tools/scripts/system_warmup.py

# å•Ÿå‹•ä¸»è¦åˆ†æ
python tools/scripts/production_analysis.py \
    --config config/production.json \
    --provider openai \
    --batch-size 10 \
    --enable-monitoring \
    --cost-limit 50.0
```

**2. ç›£æ§å’Œè­¦å ±è¨­å®š**

```python
# ç›£æ§è…³æœ¬
def setup_monitoring():
    """è¨­å®šç³»çµ±ç›£æ§"""

    # æ€§èƒ½ç›£æ§
    monitor = PerformanceMonitor()

    # æˆæœ¬æ§åˆ¶
    cost_controller = CostController(monthly_budget=100.0)

    # å¥åº·æª¢æŸ¥
    health_checker = SystemHealthChecker()

    return monitor, cost_controller, health_checker
```

**3. éŒ¯èª¤æ¢å¾©ç­–ç•¥**

```python
class RobustAnalysisSystem:
    """
    ç©©å¥çš„åˆ†æç³»çµ± - å…·å‚™å®Œæ•´çš„éŒ¯èª¤æ¢å¾©èƒ½åŠ›

    ç‰¹è‰²ï¼š
    - æ–·é»çºŒå‚³
    - è‡ªå‹•é‡è©¦
    - é™ç´šç­–ç•¥
    - æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
    """

    def __init__(self):
        self.checkpoint_manager = CheckpointManager()
        self.fallback_chain = ["openai", "claude", "gemini", "simulation"]

    async def resilient_batch_process(self, questions: List[ExamQuestion]):
        """å…·å‚™æ¢å¾©èƒ½åŠ›çš„æ‰¹é‡è™•ç†"""

        # æª¢æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„å·¥ä½œ
        checkpoint = self.checkpoint_manager.load_checkpoint()

        if checkpoint:
            logger.info(f"å¾æª¢æŸ¥é»æ¢å¾©ï¼Œå·²å®Œæˆ {checkpoint['completed']} é …")
            questions = questions[checkpoint['completed']:]

        results = []
        for i, question in enumerate(questions):
            try:
                result = await self._resilient_single_analysis(question)
                if result:
                    results.append(result)

                # å®šæœŸä¿å­˜æª¢æŸ¥é»
                if i % 10 == 0:
                    self.checkpoint_manager.save_checkpoint({
                        'completed': checkpoint.get('completed', 0) + i,
                        'results': results
                    })

            except Exception as e:
                logger.error(f"ç„¡æ³•æ¢å¾©çš„éŒ¯èª¤: {e}")
                continue

        return results
```

---

## ğŸ“š é€²ä¸€æ­¥æ¢ç´¢

å®Œæˆé€²éšä½¿ç”¨å¾Œï¼Œæ‚¨å¯ä»¥ï¼š

1. **[é–‹ç™¼æŒ‡å—](../dev/development-guide.md)** - æ·±å…¥äº†è§£ç³»çµ±æ¶æ§‹
2. **[API æ–‡æª”](../api/)** - æ¢ç´¢å®Œæ•´çš„ API åŠŸèƒ½
3. **[æ•…éšœæ’é™¤](./troubleshooting.md)** - è§£æ±ºè¤‡é›œå•é¡Œ
4. **è²¢ç»ä»£ç¢¼** - åƒèˆ‡ç³»çµ±æ”¹é€²

**è¨˜ä½ Linus çš„è©±ï¼š** "å¥½çš„ç³»çµ±è®“è¤‡é›œçš„ä»»å‹™è®Šå¾—ç°¡å–®ï¼Œè€Œä¸æ˜¯è®“ç°¡å–®çš„ä»»å‹™è®Šå¾—è¤‡é›œã€‚"

æœ¬é€²éšæŒ‡å—çš„æ‰€æœ‰åŠŸèƒ½éƒ½éµå¾ªé€™ä¸€åŸå‰‡ï¼Œæä¾›å¼·å¤§åŠŸèƒ½çš„åŒæ™‚ä¿æŒä½¿ç”¨çš„ç°¡æ½”æ€§ã€‚