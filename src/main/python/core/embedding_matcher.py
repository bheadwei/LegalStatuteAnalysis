#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç´”ç²¹çš„ Embedding åŒ¹é…ç³»çµ±
é¡Œç›® <-> æ³•æ¢çš„ç›´æ¥ embedding ç›¸ä¼¼åº¦æ¯”å°
"""

import json
import logging
import os
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import pandas as pd
import numpy as np
from openai import OpenAI

logger = logging.getLogger(__name__)

@dataclass
class MatchResult:
    """åŒ¹é…çµæœ"""
    question_id: str
    question_content: str
    matched_articles: List[Dict[str, Any]]
    processing_time: float

@dataclass 
class OptionMatchResult:
    """é¸é …åŒ¹é…çµæœ"""
    question_id: str
    option_letter: str
    option_content: str
    matched_articles: List[Dict[str, Any]]
    processing_time: float

class EmbeddingMatcher:
    """ç´”ç²¹çš„ Embedding åŒ¹é…å™¨"""
    
    def __init__(self, openai_api_key: str, embedding_model: str = "text-embedding-3-large"):
        self.client = OpenAI(api_key=openai_api_key)
        self.embedding_model = embedding_model
        
        # æ³•æ¢è³‡æ–™
        self.law_articles: List[Dict[str, Any]] = []
        self.law_embeddings: Optional[np.ndarray] = None
        
        logger.info(f"ğŸ¯ åˆå§‹åŒ– Embedding åŒ¹é…å™¨: {embedding_model}")

    def load_law_articles(self, csv_path: str) -> bool:
        """è¼‰å…¥æ³•æ¢è³‡æ–™ä¸¦å»ºç«‹ embeddings"""
        try:
            logger.info(f"ğŸ“Š è¼‰å…¥æ³•æ¢è³‡æ–™: {csv_path}")
            
            # è®€å– CSV
            df = pd.read_csv(csv_path, encoding='utf-8')
            logger.info(f"ğŸ“‹ è¼‰å…¥ {len(df)} æ¢æ³•è¦è³‡æ–™")
            
            # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
            self.law_articles = []
            for _, row in df.iterrows():
                article = {
                    "id": f"{row['æ³•è¦ä»£ç¢¼']}-{row['æ¢æ–‡ä¸»è™Ÿ']}-{row['æ¢æ–‡æ¬¡è™Ÿ']}" if row['æ¢æ–‡æ¬¡è™Ÿ'] > 0 else f"{row['æ³•è¦ä»£ç¢¼']}-{row['æ¢æ–‡ä¸»è™Ÿ']}",
                    "law_code": row['æ³•è¦ä»£ç¢¼'],
                    "law_name": row['æ³•è¦åç¨±'],
                    "chapter_title": row['ç« ç¯€æ¨™é¡Œ'],
                    "article_no_main": row['æ¢æ–‡ä¸»è™Ÿ'],
                    "article_no_sub": row['æ¢æ–‡æ¬¡è™Ÿ'],
                    "content": row['æ¢æ–‡å®Œæ•´å…§å®¹'],
                    "category": row.get('æ³•è¦é¡åˆ¥', ''),
                    "authority": row.get('ä¸»ç®¡æ©Ÿé—œ', '')
                }
                self.law_articles.append(article)
            
            # å»ºç«‹ embeddings
            self._build_embeddings()
            
            logger.info(f"âœ… æ³•æ¢è³‡æ–™è¼‰å…¥å®Œæˆ: {len(self.law_articles)} æ¢")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æ³•æ¢è³‡æ–™å¤±æ•—: {e}")
            return False

    def _build_embeddings(self):
        """å»ºç«‹æ³•æ¢ embeddings"""
        logger.info("ğŸ”§ å»ºç«‹æ³•æ¢ embeddings...")
        
        # æº–å‚™æ–‡æœ¬
        law_texts = []
        for article in self.law_articles:
            # ç°¡å–®çµ„åˆï¼šæ³•è¦åç¨± + æ¢æ–‡å…§å®¹
            text = f"{article['law_name']} ç¬¬{article['article_no_main']}æ¢ {article['content']}"
            law_texts.append(text)
        
        # æ‰¹æ¬¡å‘¼å« OpenAI Embeddings API
        embeddings = []
        batch_size = 100
        
        for i in range(0, len(law_texts), batch_size):
            batch = law_texts[i:i + batch_size]
            response = self.client.embeddings.create(
                input=batch,
                model=self.embedding_model
            )
            batch_embeddings = [data.embedding for data in response.data]
            embeddings.extend(batch_embeddings)
            
            logger.info(f"å®Œæˆæ‰¹æ¬¡ {i//batch_size + 1}/{(len(law_texts)-1)//batch_size + 1}")
        
        self.law_embeddings = np.array(embeddings)
        logger.info(f"âœ… å®Œæˆå»ºç«‹ {len(embeddings)} æ¢æ³•æ¢çš„ embeddings")

    def match_question(self, question_content: str, question_id: str = "", top_k: int = 1) -> MatchResult:
        """åŒ¹é…å–®ä¸€é¡Œç›®"""
        start_time = time.time()
        
        if self.law_embeddings is None:
            raise ValueError("æ³•æ¢ embeddings å°šæœªå»ºç«‹")
        
        # å–å¾—é¡Œç›® embedding
        response = self.client.embeddings.create(
            input=[question_content],
            model=self.embedding_model
        )
        question_embedding = np.array(response.data[0].embedding).reshape(1, -1)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarities = np.dot(question_embedding, self.law_embeddings.T)[0]
        
        # å–å¾— top-k çµæœ
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        matched_articles = []
        for idx in top_indices:
            article = self.law_articles[idx].copy()
            article['similarity'] = float(similarities[idx])
            matched_articles.append(article)
        
        processing_time = time.time() - start_time
        
        return MatchResult(
            question_id=question_id,
            question_content=question_content,
            matched_articles=matched_articles,
            processing_time=processing_time
        )

    def match_option(self, question_content: str, option_letter: str, option_content: str, question_id: str = "", top_k: int = 1) -> OptionMatchResult:
        """åŒ¹é…å–®ä¸€é¸é …"""
        start_time = time.time()
        
        if self.law_embeddings is None:
            raise ValueError("æ³•æ¢ embeddings å°šæœªå»ºç«‹")
        
        # çµ„åˆé¡Œç›®èˆ‡é¸é …
        combined_text = f"é¡Œç›®: {question_content}\né¸é … {option_letter}: {option_content}"
        
        # å–å¾— embedding
        response = self.client.embeddings.create(
            input=[combined_text],
            model=self.embedding_model
        )
        option_embedding = np.array(response.data[0].embedding).reshape(1, -1)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarities = np.dot(option_embedding, self.law_embeddings.T)[0]
        
        # å–å¾— top-k çµæœ
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        matched_articles = []
        for idx in top_indices:
            article = self.law_articles[idx].copy()
            article['similarity'] = float(similarities[idx])
            matched_articles.append(article)
        
        processing_time = time.time() - start_time
        
        return OptionMatchResult(
            question_id=question_id,
            option_letter=option_letter,
            option_content=option_content,
            matched_articles=matched_articles,
            processing_time=processing_time
        )

    def process_exam_questions(self, questions_file: str, output_file: str = None) -> Dict[str, Any]:
        """è™•ç†å®Œæ•´è€ƒé¡Œé›†"""
        logger.info(f"ğŸ“ è™•ç†è€ƒé¡Œé›†: {questions_file}")
        
        # è®€å–è€ƒé¡Œ
        with open(questions_file, 'r', encoding='utf-8') as f:
            exam_data = json.load(f)
        
        results = {
            "metadata": {
                "source_file": questions_file,
                "embedding_model": self.embedding_model,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_law_articles": len(self.law_articles)
            },
            "question_matches": [],
            "option_matches": [],
            "statistics": {
                "total_processing_time": 0.0,
                "questions_processed": 0,
                "options_processed": 0
            }
        }
        
        start_time = time.time()
        
        # è™•ç†é¸æ“‡é¡Œ
        mc_questions = exam_data.get('multiple_choice_section', {}).get('questions', [])
        
        for question in mc_questions:
            question_id = str(question['question_number'])
            question_content = question['content']
            
            # åŒ¹é…é¡Œç›®
            try:
                question_match = self.match_question(question_content, question_id)
                results["question_matches"].append({
                    "question_id": question_match.question_id,
                    "question_content": question_match.question_content,
                    "matched_articles": question_match.matched_articles,
                    "processing_time": question_match.processing_time
                })
                results["statistics"]["questions_processed"] += 1
                
            except Exception as e:
                logger.error(f"é¡Œç›® {question_id} åŒ¹é…å¤±æ•—: {e}")
            
            # åŒ¹é…é¸é …
            options = question.get('options', {})
            for option_letter, option_content in options.items():
                try:
                    option_match = self.match_option(question_content, option_letter, option_content, question_id)
                    results["option_matches"].append({
                        "question_id": option_match.question_id,
                        "option_letter": option_match.option_letter,
                        "option_content": option_match.option_content,
                        "matched_articles": option_match.matched_articles,
                        "processing_time": option_match.processing_time
                    })
                    results["statistics"]["options_processed"] += 1
                    
                except Exception as e:
                    logger.error(f"é¸é … {question_id}-{option_letter} åŒ¹é…å¤±æ•—: {e}")
        
        total_time = time.time() - start_time
        results["statistics"]["total_processing_time"] = total_time
        
        # ä¿å­˜çµæœ
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ çµæœå·²ä¿å­˜: {output_file}")
        
        logger.info(f"âœ… è™•ç†å®Œæˆ: {results['statistics']['questions_processed']} é¡Œç›®, {results['statistics']['options_processed']} é¸é …")
        
        return results
