#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„æ³•æ¢å°æ‡‰ç³»çµ±è™•ç†æµç¨‹

æµç¨‹ï¼š
1. PDF â†’ Markdown (MinerU)
2. Markdown â†’ JSON (LLM Parser)
3. æ³•æ¢ PDF â†’ CSV (å¦‚éœ€è¦)
4. JSON + æ³•æ¢CSV â†’ Embedding åŒ¹é…
5. åŒ¹é…çµæœ â†’ HTML å ±å‘Š
6. HTML â†’ PDF å ±å‘Š

ä½¿ç”¨æ–¹å¼ï¼š
    python tools/scripts/run_complete_pipeline.py \
        --exam_pdf data/pdfs/113å¹´ä¸å‹•ç”¢ç¶“ç´€æ³•è¦(ç¶“)-Aé»ƒæŒ¯åœ‹è€å¸«.pdf \
        --laws_csv data/laws_processed/law_articles.csv \
        --output_dir output
"""

import argparse
import json
import logging
import os
import sys
import tempfile
from pathlib import Path
from typing import Optional

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
from src.pdf_converter.core import PDFToMarkdownConverter
from src.parsing.llm_parser import (
    parse_questions_with_llm,
    parse_answers_with_llm,
    merge_qa_json,
)
from src.core_embedding.embedding_matcher import EmbeddingMatcher

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LegalStatuteAnalysisPipeline:
    """æ³•æ¢å°æ‡‰ç³»çµ±å®Œæ•´è™•ç†æµç¨‹"""

    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # å­ç›®éŒ„
        self.markdown_dir = output_dir / "markdown"
        self.json_dir = output_dir / "json"
        self.results_dir = output_dir / "results"

        for d in [self.markdown_dir, self.json_dir, self.results_dir]:
            d.mkdir(parents=True, exist_ok=True)

        # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
        load_dotenv()

        # æª¢æŸ¥ API Key
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("è«‹è¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")

        logger.info(f"è¼¸å‡ºç›®éŒ„: {self.output_dir}")

    def step1_pdf_to_markdown(self, pdf_path: Path) -> tuple[Path, Path]:
        """æ­¥é©Ÿ1: PDF è½‰æ›ç‚º Markdown"""
        logger.info("=" * 60)
        logger.info("æ­¥é©Ÿ 1: PDF â†’ Markdown (ä½¿ç”¨ MinerU)")
        logger.info("=" * 60)

        converter = PDFToMarkdownConverter()

        # ç”Ÿæˆ markdown æª”å
        markdown_filename = self.markdown_dir / f"{pdf_path.stem}.md"

        # è½‰æ› PDF
        content = converter.process_pdf(pdf_path, markdown_filename)

        logger.info(f"âœ… Markdown å·²ç”Ÿæˆ: {markdown_filename}")

        # å‡è¨­ç­”æ¡ˆä¹Ÿåœ¨åŒä¸€å€‹Markdownä¸­ï¼Œæˆ–è€…éœ€è¦åˆ†é›¢
        # é€™è£¡æˆ‘å€‘å…ˆè¿”å›åŒä¸€å€‹æª”æ¡ˆä½œç‚ºé¡Œç›®å’Œç­”æ¡ˆ
        return markdown_filename, markdown_filename

    def step2_markdown_to_json(self, questions_md: Path, answers_md: Path) -> Path:
        """æ­¥é©Ÿ2: Markdown è§£æç‚º JSON"""
        logger.info("=" * 60)
        logger.info("æ­¥é©Ÿ 2: Markdown â†’ JSON (LLM è§£æ)")
        logger.info("=" * 60)

        # è®€å–å…§å®¹
        q_content = questions_md.read_text(encoding='utf-8')

        # è™•ç†ç­”æ¡ˆï¼šå¦‚æœé¡Œè™Ÿå‰æœ‰ç­”æ¡ˆå­—æ¯ï¼Œéœ€è¦æå–
        # é€™è£¡å…ˆä½¿ç”¨ç°¡å–®çš„æ–¹å¼ï¼šå‡è¨­ç­”æ¡ˆåœ¨é¡Œç›®çš„markdownä¸­
        a_content = self._extract_answers_from_markdown(q_content)

        # LLM è§£æ
        logger.info("è§£æé¡Œç›®...")
        questions_result = parse_questions_with_llm(q_content)
        if not questions_result:
            raise RuntimeError("é¡Œç›®è§£æå¤±æ•—")

        logger.info("è§£æç­”æ¡ˆ...")
        answers_result = parse_answers_with_llm(a_content)
        if not answers_result:
            raise RuntimeError("ç­”æ¡ˆè§£æå¤±æ•—")

        # åˆä½µ
        final_json = merge_qa_json(questions_result, answers_result)

        # å„²å­˜
        output_file = self.json_dir / f"{questions_md.stem}_parsed.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_json, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… JSON å·²ç”Ÿæˆ: {output_file}")
        return output_file

    def _extract_answers_from_markdown(self, markdown_content: str) -> str:
        """å¾markdownä¸­æå–ç­”æ¡ˆ

        æ ¹æ“šç”¨æˆ¶èªªæ˜ï¼šç­”æ¡ˆåœ¨é¡Œè™Ÿå‰é¢ï¼ˆä¾‹å¦‚ï¼šA 1. é¡Œç›®å…§å®¹...ï¼‰
        """
        import re

        # å°‹æ‰¾æ ¼å¼å¦‚ "A 1." æˆ– "B 2." çš„æ¨¡å¼
        pattern = r'^([A-D])\s+(\d+)[\.\ã€]'

        answers = {}
        for line in markdown_content.split('\n'):
            match = re.match(pattern, line.strip())
            if match:
                answer_letter = match.group(1)
                question_number = match.group(2)
                answers[question_number] = answer_letter

        # è½‰æ›ç‚ºLLM ParseræœŸæœ›çš„æ ¼å¼
        if answers:
            answer_text = "ç­”æ¡ˆï¼š\n"
            for q_num, ans_letter in sorted(answers.items(), key=lambda x: int(x[0])):
                answer_text += f"{q_num}. {ans_letter}\n"
            return answer_text
        else:
            logger.warning("æœªèƒ½å¾markdownä¸­æå–ç­”æ¡ˆï¼Œå°‡å˜—è©¦LLMè‡ªå‹•è­˜åˆ¥")
            return "ç­”æ¡ˆï¼šï¼ˆéœ€è¦å¾é¡Œç›®ä¸­è­˜åˆ¥ï¼‰"

    def step3_embedding_matching(self, questions_json: Path, laws_csv: Path) -> Path:
        """æ­¥é©Ÿ3: Embedding åŒ¹é…"""
        logger.info("=" * 60)
        logger.info("æ­¥é©Ÿ 3: Embedding åŒ¹é…")
        logger.info("=" * 60)

        # åˆå§‹åŒ–åŒ¹é…å™¨
        matcher = EmbeddingMatcher(
            openai_api_key=self.api_key,
            embedding_model=os.getenv("EMBEDDING_MODEL", "text-embedding-3-large")
        )

        # è¼‰å…¥æ³•æ¢
        logger.info(f"è¼‰å…¥æ³•æ¢è³‡æ–™: {laws_csv}")
        if not matcher.load_law_articles(str(laws_csv)):
            raise RuntimeError("æ³•æ¢è¼‰å…¥å¤±æ•—")

        # è™•ç†è€ƒé¡Œ
        logger.info(f"è™•ç†è€ƒé¡Œ: {questions_json}")
        results = matcher.process_exam_questions(
            str(questions_json),
            output_file=None
        )

        # å„²å­˜çµæœ
        output_file = self.results_dir / f"{questions_json.stem}_matches.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… åŒ¹é…çµæœå·²ç”Ÿæˆ: {output_file}")
        return output_file

    def step4_generate_html_report(self, matches_json: Path, questions_json: Path) -> Path:
        """æ­¥é©Ÿ4: ç”Ÿæˆ HTML å ±å‘Š"""
        logger.info("=" * 60)
        logger.info("æ­¥é©Ÿ 4: ç”Ÿæˆ HTML å ±å‘Š")
        logger.info("=" * 60)

        # é€™è£¡éœ€è¦å¯¦ä½œHTMLç”Ÿæˆé‚è¼¯
        # å¯ä»¥ä½¿ç”¨stage2_html_generator.pyçš„é‚è¼¯
        logger.warning("HTMLå ±å‘Šç”ŸæˆåŠŸèƒ½å°šæœªå¯¦ä½œ")
        return None

    def run(self, exam_pdf: Path, laws_csv: Path):
        """é‹è¡Œå®Œæ•´æµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹æ³•æ¢å°æ‡‰ç³»çµ±å®Œæ•´æµç¨‹")
        logger.info(f"è€ƒé¡ŒPDF: {exam_pdf}")
        logger.info(f"æ³•æ¢CSV: {laws_csv}")

        try:
            # æ­¥é©Ÿ1: PDF â†’ Markdown
            questions_md, answers_md = self.step1_pdf_to_markdown(exam_pdf)

            # æ­¥é©Ÿ2: Markdown â†’ JSON
            questions_json = self.step2_markdown_to_json(questions_md, answers_md)

            # æ­¥é©Ÿ3: Embedding åŒ¹é…
            matches_json = self.step3_embedding_matching(questions_json, laws_csv)

            # æ­¥é©Ÿ4: ç”Ÿæˆå ±å‘Š
            # html_report = self.step4_generate_html_report(matches_json, questions_json)

            logger.info("=" * 60)
            logger.info("âœ… å®Œæ•´æµç¨‹åŸ·è¡ŒæˆåŠŸï¼")
            logger.info("=" * 60)
            logger.info(f"è¼¸å‡ºæª”æ¡ˆï¼š")
            logger.info(f"  - Markdown: {questions_md}")
            logger.info(f"  - JSON: {questions_json}")
            logger.info(f"  - åŒ¹é…çµæœ: {matches_json}")

            return True

        except Exception as e:
            logger.error(f"âŒ æµç¨‹åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
            return False


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description="æ³•æ¢å°æ‡‰ç³»çµ±å®Œæ•´è™•ç†æµç¨‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  python tools/scripts/run_complete_pipeline.py \\
      --exam_pdf data/pdfs/113å¹´ä¸å‹•ç”¢ç¶“ç´€æ³•è¦(ç¶“)-Aé»ƒæŒ¯åœ‹è€å¸«.pdf \\
      --laws_csv data/laws_processed/law_articles.csv \\
      --output_dir output
        """
    )

    parser.add_argument(
        '--exam_pdf',
        type=Path,
        required=True,
        help='è€ƒé¡ŒPDFæª”æ¡ˆè·¯å¾‘'
    )

    parser.add_argument(
        '--laws_csv',
        type=Path,
        required=True,
        help='æ³•æ¢CSVæª”æ¡ˆè·¯å¾‘'
    )

    parser.add_argument(
        '--output_dir',
        type=Path,
        default=Path('output'),
        help='è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­: outputï¼‰'
    )

    args = parser.parse_args()

    # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
    if not args.exam_pdf.exists():
        logger.error(f"è€ƒé¡ŒPDFä¸å­˜åœ¨: {args.exam_pdf}")
        return 1

    if not args.laws_csv.exists():
        logger.error(f"æ³•æ¢CSVä¸å­˜åœ¨: {args.laws_csv}")
        return 1

    # åŸ·è¡Œæµç¨‹
    pipeline = LegalStatuteAnalysisPipeline(args.output_dir)
    success = pipeline.run(args.exam_pdf, args.laws_csv)

    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
